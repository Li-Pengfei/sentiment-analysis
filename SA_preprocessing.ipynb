{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "* 1.loading data\n",
    "* 2.tokenizing data to the list\n",
    "* 3.build the vacabulary to integer\n",
    "* 4.padding the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\pc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dirname = 'data/aclImdb'\n",
    "# filename = 'aclImdb_v1.tar.gz'\n",
    "# locale.setlocale(locale.LC_ALL, 'C')\n",
    "# all_lines = []\n",
    "\n",
    "# if sys.version > '3':\n",
    "#     control_chars = [chr(0x85)]\n",
    "# else:\n",
    "#     control_chars = [unichr(0x85)]\n",
    "\n",
    "# # Convert text to lower-case and strip punctuation/symbols from words\n",
    "# def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "#     # Replace breaks with spaces\n",
    "#     norm_text = norm_text.replace('<br />', ' ')\n",
    "#     # Pad punctuation with spaces on both sides\n",
    "#     norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "#     return norm_text\n",
    "\n",
    "# if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "#     if not os.path.isdir(dirname):\n",
    "#         if not os.path.isfile(filename):\n",
    "#             # Download IMDB archive\n",
    "#             print(\"Downloading IMDB archive...\")\n",
    "#             url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "#             r = requests.get(url)\n",
    "#             with smart_open(filename, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "#         tar = tarfile.open(filename, mode='r')\n",
    "#         tar.extractall()\n",
    "#         tar.close()\n",
    "#     else:\n",
    "#         print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "#     # Collect & normalize test/train data\n",
    "#     print(\"Cleaning up dataset...\")\n",
    "#     folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "#     for fol in folders:\n",
    "#         temp = u''\n",
    "#         newline = \"\\n\".encode(\"utf-8\")\n",
    "#         output = fol.replace('/', '-') + '.txt'\n",
    "#         # Is there a better pattern to use?\n",
    "#         txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "#         print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "#         with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "#             for i, txt in enumerate(txt_files):\n",
    "#                 with smart_open(txt, \"rb\") as t:\n",
    "#                     one_text = t.read().decode(\"utf-8\")\n",
    "#                     for c in control_chars:\n",
    "#                         one_text = one_text.replace(c, ' ')\n",
    "#                     one_text = normalize_text(one_text)\n",
    "#                     all_lines.append(one_text)\n",
    "#                     n.write(one_text.encode(\"utf-8\"))\n",
    "#                     n.write(newline)\n",
    "\n",
    "#     # Save to disk for instant re-use on any future runs\n",
    "#     with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "#         for idx, line in enumerate(all_lines):\n",
    "#             num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "#             f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "# assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "# print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text]  # optional: convert abll words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('data/aclImdb/train-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "test_data , test_label = shuffle(test_data ,test_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  \n",
    "\n",
    "#**********for training data prepocessing\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        train_data_word[i].append(word_tokenize(train_data_sent[i][j]))  \n",
    "        \n",
    "#building the vacabulary\n",
    "vocab_to_int_train = build_vocab(train_data_word)\n",
    "\n",
    "\n",
    "#get the padding element\n",
    "maxlen_word1 = 0\n",
    "maxlen_sent1 = 0\n",
    "#pad_train_set = []\n",
    "list_maxlen_sent1 = []\n",
    "list_maxlen_word1 = []\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent1.append((len(train_data_sent[i])))\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word1.append(len(train_data_sent[i][j]))\n",
    "#get the max sentence\n",
    "list_maxlen_sent1 = sorted(list_maxlen_sent1)\n",
    "maxlen_sent1 = list_maxlen_sent1[int(len(list_maxlen_sent1)*0.95)]\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word1)\n",
    "maxlen_word1 = list_maxlen_word1[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "\n",
    "#start to pad\n",
    "train_copus_padded = build_input_data(corpus=train_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# **********for testing data prepocessing\n",
    "# tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "test_data_word = [[]for i in range(len(test_data_sent))]\n",
    "for i in range(len(test_data_sent)):\n",
    "    for j in range(len(test_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        test_data_word[i].append(word_tokenize(test_data_sent[i][j]))  \n",
    "        \n",
    "# #building the vacabulary\n",
    "# vocab_to_int_test = build_vocab(test_data_word)\n",
    "\n",
    "#start to pad test data\n",
    "#25000*36*224\n",
    "test_copus_padded = build_input_data(corpus=test_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 36, 224)\n",
      "(25000, 36, 224)\n",
      "[\"the plot of this terrible film is so convoluted i've put the spoiler warning up because i'm unsure if i'm giving anything away .\", 'the audience first sees some man in jack the ripper garb murder an old man in an alley a hundred years ago .', \"then we're up to modern day and a young australian couple is looking for a house .\", \"we're given an unbelievably long tour of this house and the husband sees a figure in an old mirror .\", 'some 105 year old woman lived there .', 'there are also large iron panels covering a wall in the den .', \"an old fashioned straight-razor falls out when they're renovating and the husband keeps it .\", 'i guess he becomes possessed by the razor because he starts having weird dreams .', 'oh yeah ,  the couple is unable to have a baby because the husband is firing blanks .', 'some mold seems to be climbing up the wall after the couple removes the iron panels and the mold has the shape of a person .', 'late in the story there is a plot about a large cache of money & the husband murders the body guard & a co-worker and steals the money .', 'his wife is suddenly pregnant .', 'what the hell is going on ?', '?', 'who knows ?', '?', 'nothing is explained .', 'was the 105 year old woman the child of the serial killer ?', 'the baby sister ?', 'why were iron panels put on the wall ?', 'how would that keep the serial killer contained in the cellar ?', 'was he locked down there by his family & starved to death or just concealed ?', 'who is mr .', 'hobbs and why is he so desperate to get the iron panels ?', '?', \"he's never seen again .\", 'why was the serial killer killing people ?', 'we only see the one old man murdered .', 'was there a pattern or motive or something ?', '?', 'why does the wife suddenly become pregnant ?', 'is it the demon spawn of the serial killer ?', \"has he managed to infiltrate the husband's semen ?\", 'and why ,  if the husband was able to subdue and murder a huge ,  burly security guard ,  is he unable to overpower his wife ?', 'and just how powerful is the voltage system in australia that it would knock him across the room simply cutting a light wire ?', 'and why does the wife stay in the house ?', 'is she now possessed by the serial killer ?', 'is the baby going to be the killer reincarnated ?', 'this movie was such a frustrating experience i wanted to call my pbs station and ask for my money back !', \"the only enjoyable aspect of this story was seeing the husband running around in just his boxer shorts for a lot of the time ,  but even that couldn't redeem this muddled ,  incoherent mess .\"] \n",
      "\n",
      "[['the', 'plot', 'of', 'this', 'terrible', 'film', 'is', 'so', 'convoluted', 'i', \"'ve\", 'put', 'the', 'spoiler', 'warning', 'up', 'because', 'i', \"'m\", 'unsure', 'if', 'i', \"'m\", 'giving', 'anything', 'away', '.'], ['the', 'audience', 'first', 'sees', 'some', 'man', 'in', 'jack', 'the', 'ripper', 'garb', 'murder', 'an', 'old', 'man', 'in', 'an', 'alley', 'a', 'hundred', 'years', 'ago', '.'], ['then', 'we', \"'re\", 'up', 'to', 'modern', 'day', 'and', 'a', 'young', 'australian', 'couple', 'is', 'looking', 'for', 'a', 'house', '.'], ['we', \"'re\", 'given', 'an', 'unbelievably', 'long', 'tour', 'of', 'this', 'house', 'and', 'the', 'husband', 'sees', 'a', 'figure', 'in', 'an', 'old', 'mirror', '.'], ['some', '105', 'year', 'old', 'woman', 'lived', 'there', '.'], ['there', 'are', 'also', 'large', 'iron', 'panels', 'covering', 'a', 'wall', 'in', 'the', 'den', '.'], ['an', 'old', 'fashioned', 'straight-razor', 'falls', 'out', 'when', 'they', \"'re\", 'renovating', 'and', 'the', 'husband', 'keeps', 'it', '.'], ['i', 'guess', 'he', 'becomes', 'possessed', 'by', 'the', 'razor', 'because', 'he', 'starts', 'having', 'weird', 'dreams', '.'], ['oh', 'yeah', ',', 'the', 'couple', 'is', 'unable', 'to', 'have', 'a', 'baby', 'because', 'the', 'husband', 'is', 'firing', 'blanks', '.'], ['some', 'mold', 'seems', 'to', 'be', 'climbing', 'up', 'the', 'wall', 'after', 'the', 'couple', 'removes', 'the', 'iron', 'panels', 'and', 'the', 'mold', 'has', 'the', 'shape', 'of', 'a', 'person', '.'], ['late', 'in', 'the', 'story', 'there', 'is', 'a', 'plot', 'about', 'a', 'large', 'cache', 'of', 'money', '&', 'the', 'husband', 'murders', 'the', 'body', 'guard', '&', 'a', 'co-worker', 'and', 'steals', 'the', 'money', '.'], ['his', 'wife', 'is', 'suddenly', 'pregnant', '.'], ['what', 'the', 'hell', 'is', 'going', 'on', '?'], ['?'], ['who', 'knows', '?'], ['?'], ['nothing', 'is', 'explained', '.'], ['was', 'the', '105', 'year', 'old', 'woman', 'the', 'child', 'of', 'the', 'serial', 'killer', '?'], ['the', 'baby', 'sister', '?'], ['why', 'were', 'iron', 'panels', 'put', 'on', 'the', 'wall', '?'], ['how', 'would', 'that', 'keep', 'the', 'serial', 'killer', 'contained', 'in', 'the', 'cellar', '?'], ['was', 'he', 'locked', 'down', 'there', 'by', 'his', 'family', '&', 'starved', 'to', 'death', 'or', 'just', 'concealed', '?'], ['who', 'is', 'mr', '.'], ['hobbs', 'and', 'why', 'is', 'he', 'so', 'desperate', 'to', 'get', 'the', 'iron', 'panels', '?'], ['?'], ['he', \"'s\", 'never', 'seen', 'again', '.'], ['why', 'was', 'the', 'serial', 'killer', 'killing', 'people', '?'], ['we', 'only', 'see', 'the', 'one', 'old', 'man', 'murdered', '.'], ['was', 'there', 'a', 'pattern', 'or', 'motive', 'or', 'something', '?'], ['?'], ['why', 'does', 'the', 'wife', 'suddenly', 'become', 'pregnant', '?'], ['is', 'it', 'the', 'demon', 'spawn', 'of', 'the', 'serial', 'killer', '?'], ['has', 'he', 'managed', 'to', 'infiltrate', 'the', 'husband', \"'s\", 'semen', '?'], ['and', 'why', ',', 'if', 'the', 'husband', 'was', 'able', 'to', 'subdue', 'and', 'murder', 'a', 'huge', ',', 'burly', 'security', 'guard', ',', 'is', 'he', 'unable', 'to', 'overpower', 'his', 'wife', '?'], ['and', 'just', 'how', 'powerful', 'is', 'the', 'voltage', 'system', 'in', 'australia', 'that', 'it', 'would', 'knock', 'him', 'across', 'the', 'room', 'simply', 'cutting', 'a', 'light', 'wire', '?'], ['and', 'why', 'does', 'the', 'wife', 'stay', 'in', 'the', 'house', '?'], ['is', 'she', 'now', 'possessed', 'by', 'the', 'serial', 'killer', '?'], ['is', 'the', 'baby', 'going', 'to', 'be', 'the', 'killer', 'reincarnated', '?'], ['this', 'movie', 'was', 'such', 'a', 'frustrating', 'experience', 'i', 'wanted', 'to', 'call', 'my', 'pbs', 'station', 'and', 'ask', 'for', 'my', 'money', 'back', '!'], ['the', 'only', 'enjoyable', 'aspect', 'of', 'this', 'story', 'was', 'seeing', 'the', 'husband', 'running', 'around', 'in', 'just', 'his', 'boxer', 'shorts', 'for', 'a', 'lot', 'of', 'the', 'time', ',', 'but', 'even', 'that', 'could', \"n't\", 'redeem', 'this', 'muddled', ',', 'incoherent', 'mess', '.']] \n",
      "\n",
      "[[  2 126   7 ...   0   0   0]\n",
      " [  2 310  99 ...   0   0   0]\n",
      " [105  78 187 ...   0   0   0]\n",
      " ...\n",
      " [  5 147   4 ...   0   0   0]\n",
      " [  5  49 100 ...   0   0   0]\n",
      " [  5 147  81 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_copus_padded.shape)\n",
    "print(test_copus_padded.shape)\n",
    "\n",
    "print(train_data_sent[8], '\\n')\n",
    "print(train_data_word[8], '\\n')\n",
    "print(train_copus_padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the pickle to store the data\n",
    "file = open('pickle_data/train_label.pickle','wb')\n",
    "pickle.dump(train_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','wb')\n",
    "pickle.dump(test_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_data_word.pickle','wb')\n",
    "pickle.dump(train_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_data_word.pickle','wb')\n",
    "pickle.dump(test_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_data_sent.pickle','wb')\n",
    "pickle.dump(train_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_data_sent.pickle','wb')\n",
    "pickle.dump(test_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','wb')\n",
    "pickle.dump(vocab_to_int_train,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_copus_pad.pickle','wb')\n",
    "pickle.dump(train_copus_padded,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','wb')\n",
    "pickle.dump(test_copus_padded,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding matrix\n",
    "#the number of the vocabulary is 100327\n",
    "#把每个词映射到一个300维度的vector\n",
    "#这个matrix是二维的\n",
    "#用vocab2int中每个词对应的整数来去matrix来找对应的vector\n",
    "dimension = 300\n",
    "path = 'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
    "embedding_matrix = load_embedding_matrix_gensim(embed_path = path,vocab2int=vocab_to_int_train,EMBEDDING_DIM=dimension)\n",
    "\n",
    "#use pickle to store the data\n",
    "file = open('pickle_data/embedding_matrix','wb')\n",
    "pickle.dump(embedding_matrix,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100327, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_to_int_train))\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
