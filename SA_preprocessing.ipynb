{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "* 1.loading data\n",
    "* 2.tokenizing data to the list\n",
    "* 3.build the vacabulary to integer\n",
    "* 4.padding the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      " train/pos: 12500 files\n",
      " train/neg: 12500 files\n",
      " test/pos: 12500 files\n",
      " test/neg: 12500 files\n",
      "Success, alldata-id.txt is available for next steps.\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\[\\].\\\",()!?;:/])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('data/aclImdb/alldata-id.txt'):\n",
    "#     if not os.path.isdir(dirname):\n",
    "#         if not os.path.isfile(filename):\n",
    "#             # Download IMDB archive\n",
    "#             print(\"Downloading IMDB archive...\")\n",
    "#             url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "#             r = requests.get(url)\n",
    "#             with smart_open(filename, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "#         tar = tarfile.open(filename, mode='r')\n",
    "#         tar.extractall()\n",
    "#         tar.close()\n",
    "#     else:\n",
    "#         print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline = \"\\n\".encode(\"utf-8\")\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('data/aclImdb/train-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "test_data , test_label = shuffle(test_data ,test_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  \n",
    "\n",
    "#**********for training data prepocessing\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        word_tokens = word_tokenize(train_data_sent[i][j])\n",
    "        if word_tokens != []:\n",
    "            train_data_word[i].append(word_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ,  I watched the films .  .  .  I thought ,   \" Why the heck was this film such a high success in the Korean Box Office ?  \"  Even thought the movie had a clever / unusal scenario ,  the acting wasn't that good and the characters weren't very interesting .  For a Korean movie .  .  .  I liked the fighting scenes .  If you want to watch a film without thinking ,  this is the film for you .  But I got to admit .  .  .  the film was kind of childish .  .  .  6 / 10\n",
      "[['after', 'i', 'watched', 'the', 'films'], ['i', 'thought', '``', 'why', 'the', 'heck', 'was', 'this', 'film', 'such', 'a', 'high', 'success', 'in', 'the', 'korean', 'box', 'office', '``'], ['even', 'thought', 'the', 'movie', 'had', 'a', 'clever', 'unusal', 'scenario', 'the', 'acting', 'was', \"n't\", 'that', 'good', 'and', 'the', 'characters', 'were', \"n't\", 'very', 'interesting'], ['for', 'a', 'korean', 'movie'], ['i', 'liked', 'the', 'fighting', 'scenes'], ['if', 'you', 'want', 'to', 'watch', 'a', 'film', 'without', 'thinking', 'this', 'is', 'the', 'film', 'for', 'you'], ['but', 'i', 'got', 'to', 'admit'], ['the', 'film', 'was', 'kind', 'of', 'childish'], ['6', '10']]\n"
     ]
    }
   ],
   "source": [
    "t_idx = 9\n",
    "print(train_data[t_idx])\n",
    "print(train_data_word[t_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int_train = build_vocab(train_data_word)\n",
    "\n",
    "#get the padding element\n",
    "maxlen_word1 = 0\n",
    "maxlen_sent1 = 0\n",
    "#pad_train_set = []\n",
    "list_maxlen_sent1 = []\n",
    "list_maxlen_word1 = []\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent1.append((len(train_data_sent[i])))\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word1.append(len(train_data_sent[i][j]))\n",
    "#get the max sentence\n",
    "list_maxlen_sent1 = sorted(list_maxlen_sent1)\n",
    "maxlen_sent1 = list_maxlen_sent1[int(len(list_maxlen_sent1)*0.95)]\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word1)\n",
    "maxlen_word1 = list_maxlen_word1[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "\n",
    "#start to pad\n",
    "train_copus_padded = build_input_data(corpus=train_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********for testing data prepocessing\n",
    "# tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "test_data_word = [[]for i in range(len(test_data_sent))]\n",
    "for i in range(len(test_data_sent)):\n",
    "    for j in range(len(test_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        test_data_word[i].append(word_tokenize(test_data_sent[i][j]))  \n",
    "        \n",
    "# #building the vacabulary\n",
    "# vocab_to_int_test = build_vocab(test_data_word)\n",
    "\n",
    "#start to pad test data\n",
    "#25000*36*224\n",
    "test_copus_padded = build_input_data(corpus=test_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 36, 224)\n",
      "(25000, 36, 224)\n",
      "['This is a low budget Roger Corman horror / creature flick .', 'A DinoCroc is created when manipulation of prehistoric genes runs amok .', 'An engineered croc first kills one of its own then gets the taste of human and becomes a fast growing terror after escaping .', 'None of the characters have any depth ,  but then they are not the focal point .', 'We only get a few glimpses of the huge two-legged dinosaur descendant and some of the best  \" kill \"  scenes in a small budget film .', 'My favorite scene is of a moronic character trying to use a three legged dog for bait and becomes croc food himself .', 'Nothing left on the pier but ankle top feet .', 'With no real stand out roles :  Jane Longendecker ,  Bruce Weitz and Charles Napier .', 'Most pathetic is Matt Borlenghi and an obnoxious professional croc hunter Costas Mandylor .', 'I was most impressed with the alluring Joanna Pacula as the respectfully feared Dr .', 'P .', 'DINOCROC is redeeming as a crock of pickles .'] \n",
      "\n",
      "[['this', 'is', 'a', 'low', 'budget', 'roger', 'corman', 'horror', 'creature', 'flick'], ['a', 'dinocroc', 'is', 'created', 'when', 'manipulation', 'of', 'prehistoric', 'genes', 'runs', 'amok'], ['an', 'engineered', 'croc', 'first', 'kills', 'one', 'of', 'its', 'own', 'then', 'gets', 'the', 'taste', 'of', 'human', 'and', 'becomes', 'a', 'fast', 'growing', 'terror', 'after', 'escaping'], ['none', 'of', 'the', 'characters', 'have', 'any', 'depth', 'but', 'then', 'they', 'are', 'not', 'the', 'focal', 'point'], ['we', 'only', 'get', 'a', 'few', 'glimpses', 'of', 'the', 'huge', 'two-legged', 'dinosaur', 'descendant', 'and', 'some', 'of', 'the', 'best', '``', 'kill', '``', 'scenes', 'in', 'a', 'small', 'budget', 'film'], ['my', 'favorite', 'scene', 'is', 'of', 'a', 'moronic', 'character', 'trying', 'to', 'use', 'a', 'three', 'legged', 'dog', 'for', 'bait', 'and', 'becomes', 'croc', 'food', 'himself'], ['nothing', 'left', 'on', 'the', 'pier', 'but', 'ankle', 'top', 'feet'], ['with', 'no', 'real', 'stand', 'out', 'roles', 'jane', 'longendecker', 'bruce', 'weitz', 'and', 'charles', 'napier'], ['most', 'pathetic', 'is', 'matt', 'borlenghi', 'and', 'an', 'obnoxious', 'professional', 'croc', 'hunter', 'costas', 'mandylor'], ['i', 'was', 'most', 'impressed', 'with', 'the', 'alluring', 'joanna', 'pacula', 'as', 'the', 'respectfully', 'feared', 'dr'], ['p'], ['dinocroc', 'is', 'redeeming', 'as', 'a', 'crock', 'of', 'pickles']] \n",
      "\n",
      "[[    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]\n",
      " ...\n",
      " [   10    15    93 ...     0     0     0]\n",
      " [ 1841     0     0 ...     0     0     0]\n",
      " [12895     7  1609 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_copus_padded.shape)\n",
    "print(test_copus_padded.shape)\n",
    "\n",
    "print(train_data_sent[8], '\\n')\n",
    "print(train_data_word[8], '\\n')\n",
    "print(train_copus_padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the pickle to store the data\n",
    "file = open('pickle_data/train_label.pickle','wb')\n",
    "pickle.dump(train_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','wb')\n",
    "pickle.dump(test_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_data_word.pickle','wb')\n",
    "pickle.dump(train_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_data_word.pickle','wb')\n",
    "pickle.dump(test_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_data_sent.pickle','wb')\n",
    "pickle.dump(train_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_data_sent.pickle','wb')\n",
    "pickle.dump(test_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','wb')\n",
    "pickle.dump(vocab_to_int_train,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train_copus_pad.pickle','wb')\n",
    "pickle.dump(train_copus_padded,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','wb')\n",
    "pickle.dump(test_copus_padded,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the embedding matrix\n",
    "#the number of the vocabulary is 100327\n",
    "#把每个词映射到一个300维度的vector\n",
    "#这个matrix是二维的\n",
    "#用vocab2int中每个词对应的整数来去matrix来找对应的vector\n",
    "dimension = 300\n",
    "path = '~/Desktop/NLP_resources/GoogleNews-vectors-negative300.bin'\n",
    "embedding_matrix = load_embedding_matrix_gensim(embed_path = path,vocab2int=vocab_to_int_train,EMBEDDING_DIM=dimension)\n",
    "\n",
    "#use pickle to store the data\n",
    "file = open('pickle_data/embedding_matrix','wb')\n",
    "pickle.dump(embedding_matrix,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97162, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab_to_int_train))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
