{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* 获取senti words\n",
    "* 别忘了clean里面的符号（用clean代替一些代码）\n",
    "* 拿了大概五百个\n",
    "* filter weights用sent2net里的词汇\n",
    "* 从sentiwordnet,如何从文件里抓取极性词，分数大于0.8，建立两个得到list的情感词。分开positive和negative\n",
    "* 加到一个list\n",
    "* layer = return true\n",
    "\n",
    "* 自己得到cnn_filter weights\n",
    "* 默认train的时候得到weights是initilizer randomly的\n",
    "* 自己train的时候，trainable为false，因为已经得到情感词\n",
    "* weight.shape（window_size,feature_dimension(embedding),number of words/number of filter）\n",
    "* bias.shape(number of sentiment words/number of filter)一维list \n",
    "* 两个合在一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "* function:get the sentiment words,use the filters to get\n",
    "* input:()\n",
    "* filter:to extract some features\n",
    "* filter_length:the quantity of filters\n",
    "* max_pooling:\n",
    "* output:(batch_size*)\n",
    "* 用两个CNN，一个是knowledge的，如果没找完，就要用random了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "* function:\n",
    "* input:\n",
    "* output:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions\n",
    "* 为什么maxpooling得到这个\n",
    "* 可以试试别的\n",
    "* filter的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "* 两个最重要的是vocab2int 和 embedding matrix\n",
    "* 纯CNN跑，用全部的数据，不用分句\n",
    "* 后面三个是以句子为单位\n",
    "* cnn layer2是基于知识的\n",
    "* 用skLearn得到recall 和precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 36, 224) (25000, 36, 224)\n",
      "embedding_matrix shape: (97162, 300)\n",
      "vocabulary size: 97162\n",
      "max sent length: 36 \n",
      "max word length: 224\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "maxlen_sent = train_copus_padded.shape[1]\n",
    "maxlen_word = train_copus_padded.shape[2]\n",
    "print('max sent length:', maxlen_sent, '\\nmax word length:', maxlen_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sentiment words from SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'resource/SentiWordNet_3.0.0_20130122.txt'\n",
    "content = []\n",
    "with open(file_path,'r',encoding='UTF-8') as f:\n",
    "    while(1):\n",
    "        lines = f.readline()\n",
    "        if(lines[0] is ('a' or 'n' or 'v' or 'r')):\n",
    "            content.append(lines.split('\\t'))\n",
    "        elif(lines[0] is '#'):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "             \n",
    "#get the sentiment words(score>0.8)\n",
    "neg_words1 = []\n",
    "pos_words1 = []\n",
    "for lines in content:\n",
    "    if(float(lines[2])>=0.8):\n",
    "        pos_words1.append(lines[4])\n",
    "    elif(float(lines[3])>=0.8):\n",
    "        neg_words1.append(lines[4])\n",
    "\n",
    "#get the positive sentiment words\n",
    "#seperate the multiple words\n",
    "pos_words2 = []\n",
    "for i in range(len(pos_words1)):\n",
    "    pos_words2.append(pos_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "pos_words3 = []\n",
    "for i in pos_words2:\n",
    "    for j in i:\n",
    "        pos_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "pos_words4 = []\n",
    "for elem in pos_words3:\n",
    "    pos_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "pos_lexicon = []\n",
    "for elem in pos_words4:\n",
    "    pos_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "    \n",
    "#get the negative sentiment words\n",
    "#seperate the multiple words\n",
    "neg_words2 = []\n",
    "for i in range(len(neg_words1)):\n",
    "    neg_words2.append(neg_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "neg_words3 = []\n",
    "for i in neg_words2:\n",
    "    for j in i:\n",
    "        neg_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "neg_words4 = []\n",
    "for elem in neg_words3:\n",
    "    neg_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "neg_lexicon = []\n",
    "for elem in neg_words4:\n",
    "    neg_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "#merge the positive and negative words\n",
    "senti_lexicon = pos_lexicon + neg_lexicon\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "senti_lexicon = shuffle(senti_lexicon,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(senti_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map the sentiment words to integer based on vocab2int\n",
    "senti2int = [getWordIdx(word, vocab_to_int_train) for word in senti_lexicon]\n",
    "\n",
    "#get the filter weights based on the sentiment words&vocab2int&embedding_matrix\n",
    "def Find_Filter_Weight(senti2int):\n",
    "    \"\"\"sentiwords is the list\"\"\"\n",
    "    word_filter_weights = []\n",
    "    bias_weights = []\n",
    "    filter_len = 1\n",
    "    for i in senti2int:\n",
    "        vector = embedding_matrix[i]  # shape: 300\n",
    "        vector = np.expand_dims(vector, axis=0) #shape: 1x 300\n",
    "        vector = np.expand_dims(vector, axis=2) #shape: 1x 300 x 1\n",
    "        if len(word_filter_weights) == 0:\n",
    "            word_filter_weights = vector\n",
    "        else:\n",
    "            word_filter_weights = np.concatenate((word_filter_weights, vector), axis=2)   \n",
    "    #shape is (1, 300, 533)\n",
    "    \n",
    "    bias_weights = np.zeros(len(senti2int))\n",
    "    cnn_wordfilter_weights = [word_filter_weights, bias_weights]\n",
    "    \n",
    "    return cnn_wordfilter_weights    "
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 533)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_weights = Find_Filter_Weight(senti2int)\n",
    "CNN_weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment \n",
    "* record the result of running the different vesions\n",
    "* change the parameter to run and record the result again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data prepocessing for CNN/GRU/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('data/aclImdb/train-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "test_data , test_label = shuffle(test_data ,test_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data for the cnn layer\n",
    "#word tokenizing the data\n",
    "training_data = []\n",
    "for data in train_data:\n",
    "    training_data.append(word_tokenize(data))\n",
    "\n",
    "testing_data = []\n",
    "for data in test_data:\n",
    "    testing_data.append(word_tokenize(data))\n",
    "    \n",
    "#map the word to integer\n",
    "x_train = []\n",
    "word2int = []\n",
    "for review in training_data:\n",
    "    word2int =[getWordIdx(word, vocab_to_int_train) for word in review]\n",
    "    x_train.append(word2int)\n",
    "    \n",
    "x_test = []\n",
    "word2int = []\n",
    "for review in testing_data:\n",
    "    word2int = [getWordIdx(word, vocab_to_int_train) for word in review]\n",
    "    x_test.append(word2int)\n",
    "\n",
    "\n",
    "#show and padding the data\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "sequence = keras.preprocessing.sequence\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_word)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_word)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:13: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=100, activation=\"tanh\", padding=\"same\", kernel_size=3, strides=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 224, 300)          30089100  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 224, 100)          90100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,184,301\n",
      "Trainable params: 95,201\n",
      "Non-trainable params: 30,089,100\n",
      "_________________________________________________________________\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 133s 5ms/step - loss: 0.4593 - acc: 0.7974 - val_loss: 0.3243 - val_acc: 0.8607\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 127s 5ms/step - loss: 0.2855 - acc: 0.8840 - val_loss: 0.2897 - val_acc: 0.8752\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.2226 - acc: 0.9150 - val_loss: 0.2734 - val_acc: 0.8829\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.1716 - acc: 0.9392 - val_loss: 0.2717 - val_acc: 0.8886\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 195s 8ms/step - loss: 0.1190 - acc: 0.9642 - val_loss: 0.3002 - val_acc: 0.8800\n",
      "25000/25000 [==============================] - 46s 2ms/step\n",
      "Test accuracy: 0.8800399992465973\n"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "num_filter = 100\n",
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = Convolution1D(nb_filter=num_filter,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1\n",
    "                            )(embeded)\n",
    "cnn_out = GlobalMaxPooling1D()(hidden_layer)\n",
    "dense = Dense(50, activation='sigmoid')(cnn_out)\n",
    "#dense = Dense(dim, activation='softmax')(cnn_out)\n",
    "Final = Dense(1,activation = 'sigmoid')(dense)#为什么换成softmax就很差\n",
    "model = Model(inputs=input_data, outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)\n",
    "#the validate is too much?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 44s 2ms/step\n",
      "Accuracy: 0.8800\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9221    0.8302    0.8738     12500\n",
      "          1     0.8456    0.9298    0.8857     12500\n",
      "\n",
      "avg / total     0.8838    0.8800    0.8797     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classical LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 224, 300)          30089100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,315,249\n",
      "Trainable params: 226,149\n",
      "Non-trainable params: 30,089,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build LSTM model\n",
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "# hidden_layer = LSTM(128, activation = 'sigmoid',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "#hidden_layer = LSTM(128, activation = 'relu',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "# hidden_layer = LSTM(128, activation = 'tanh',dropout=0.1, recurrent_dropout=0.2)(embeded)\n",
    "hidden_layer = LSTM(128, activation = 'tanh',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "dense = Dense(50,activation = 'sigmoid')(hidden_layer)\n",
    "Final = Dense(1,activation = 'sigmoid')(dense)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 461s 18ms/step - loss: 0.5767 - acc: 0.6963 - val_loss: 0.4569 - val_acc: 0.8012\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 428s 17ms/step - loss: 0.5340 - acc: 0.7453 - val_loss: 0.5031 - val_acc: 0.7663\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 436s 17ms/step - loss: 0.5433 - acc: 0.7238 - val_loss: 0.4176 - val_acc: 0.8129\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 436s 17ms/step - loss: 0.4518 - acc: 0.7963 - val_loss: 0.3764 - val_acc: 0.8390\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 436s 17ms/step - loss: 0.3924 - acc: 0.8293 - val_loss: 0.3516 - val_acc: 0.8545\n",
      "25000/25000 [==============================] - 111s 4ms/step\n",
      "Test accuracy: 0.8545199971199036\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 111s 4ms/step\n",
      "Accuracy: 0.8545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8756    0.8264    0.8503     12500\n",
      "          1     0.8356    0.8826    0.8585     12500\n",
      "\n",
      "avg / total     0.8556    0.8545    0.8544     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    \n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classical GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 224, 300)          30089100  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               164736    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 30,260,337\n",
      "Trainable params: 171,237\n",
      "Non-trainable params: 30,089,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build GRU model\n",
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = GRU(128, activation = 'tanh',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "dense = Dense(50,activation = 'sigmoid')(hidden_layer)\n",
    "Final = Dense(1,activation = 'sigmoid')(dense)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 339s 14ms/step - loss: 0.5899 - acc: 0.6816 - val_loss: 0.4809 - val_acc: 0.7717\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 326s 13ms/step - loss: 0.4704 - acc: 0.7861 - val_loss: 0.4959 - val_acc: 0.7667\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 331s 13ms/step - loss: 0.4066 - acc: 0.8232 - val_loss: 0.3409 - val_acc: 0.8564\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 327s 13ms/step - loss: 0.3573 - acc: 0.8480 - val_loss: 0.3231 - val_acc: 0.8612\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 327s 13ms/step - loss: 0.3240 - acc: 0.8638 - val_loss: 0.2922 - val_acc: 0.8765\n",
      "25000/25000 [==============================] - 94s 4ms/step\n",
      "Test accuracy: 0.8765200004577637\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 95s 4ms/step\n",
      "Accuracy: 0.8765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8963    0.8516    0.8734     12500\n",
      "          1     0.8586    0.9014    0.8795     12500\n",
      "\n",
      "avg / total     0.8775    0.8765    0.8764     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCNN+GRU(The document is seperated by sentence first and then by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the kCNN+GRU model\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word))\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "cnn_layer1 = Convolution1D(nb_filter=50,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                           weights = CNN_weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "cnn_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    \n",
    "    sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "    \n",
    "    cnn_out.append(sent_cnn)\n",
    "cnn_out = stack_layer(cnn_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "\n",
    "#处理句子，从头到尾\n",
    "gru= GRU(256, dropout=0.3, recurrent_dropout=0.2)(cnn_out)\n",
    "dense1 = Dense(128, activation='sigmoid')(gru)\n",
    "Final = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "batch_size = 300\n",
    "num_epoch = 5\n",
    "\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### GRU+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 36, 224)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 224, 300)     29148600    lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 128)          164736      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "                                                                 embedding_1[3][0]                \n",
      "                                                                 embedding_1[4][0]                \n",
      "                                                                 embedding_1[5][0]                \n",
      "                                                                 embedding_1[6][0]                \n",
      "                                                                 embedding_1[7][0]                \n",
      "                                                                 embedding_1[8][0]                \n",
      "                                                                 embedding_1[9][0]                \n",
      "                                                                 embedding_1[10][0]               \n",
      "                                                                 embedding_1[11][0]               \n",
      "                                                                 embedding_1[12][0]               \n",
      "                                                                 embedding_1[13][0]               \n",
      "                                                                 embedding_1[14][0]               \n",
      "                                                                 embedding_1[15][0]               \n",
      "                                                                 embedding_1[16][0]               \n",
      "                                                                 embedding_1[17][0]               \n",
      "                                                                 embedding_1[18][0]               \n",
      "                                                                 embedding_1[19][0]               \n",
      "                                                                 embedding_1[20][0]               \n",
      "                                                                 embedding_1[21][0]               \n",
      "                                                                 embedding_1[22][0]               \n",
      "                                                                 embedding_1[23][0]               \n",
      "                                                                 embedding_1[24][0]               \n",
      "                                                                 embedding_1[25][0]               \n",
      "                                                                 embedding_1[26][0]               \n",
      "                                                                 embedding_1[27][0]               \n",
      "                                                                 embedding_1[28][0]               \n",
      "                                                                 embedding_1[29][0]               \n",
      "                                                                 embedding_1[30][0]               \n",
      "                                                                 embedding_1[31][0]               \n",
      "                                                                 embedding_1[32][0]               \n",
      "                                                                 embedding_1[33][0]               \n",
      "                                                                 embedding_1[34][0]               \n",
      "                                                                 embedding_1[35][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 36, 128)      0           gru_1[0][0]                      \n",
      "                                                                 gru_1[1][0]                      \n",
      "                                                                 gru_1[2][0]                      \n",
      "                                                                 gru_1[3][0]                      \n",
      "                                                                 gru_1[4][0]                      \n",
      "                                                                 gru_1[5][0]                      \n",
      "                                                                 gru_1[6][0]                      \n",
      "                                                                 gru_1[7][0]                      \n",
      "                                                                 gru_1[8][0]                      \n",
      "                                                                 gru_1[9][0]                      \n",
      "                                                                 gru_1[10][0]                     \n",
      "                                                                 gru_1[11][0]                     \n",
      "                                                                 gru_1[12][0]                     \n",
      "                                                                 gru_1[13][0]                     \n",
      "                                                                 gru_1[14][0]                     \n",
      "                                                                 gru_1[15][0]                     \n",
      "                                                                 gru_1[16][0]                     \n",
      "                                                                 gru_1[17][0]                     \n",
      "                                                                 gru_1[18][0]                     \n",
      "                                                                 gru_1[19][0]                     \n",
      "                                                                 gru_1[20][0]                     \n",
      "                                                                 gru_1[21][0]                     \n",
      "                                                                 gru_1[22][0]                     \n",
      "                                                                 gru_1[23][0]                     \n",
      "                                                                 gru_1[24][0]                     \n",
      "                                                                 gru_1[25][0]                     \n",
      "                                                                 gru_1[26][0]                     \n",
      "                                                                 gru_1[27][0]                     \n",
      "                                                                 gru_1[28][0]                     \n",
      "                                                                 gru_1[29][0]                     \n",
      "                                                                 gru_1[30][0]                     \n",
      "                                                                 gru_1[31][0]                     \n",
      "                                                                 gru_1[32][0]                     \n",
      "                                                                 gru_1[33][0]                     \n",
      "                                                                 gru_1[34][0]                     \n",
      "                                                                 gru_1[35][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 36, 100)      38500       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           5050        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            51          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 29,356,937\n",
      "Trainable params: 208,337\n",
      "Non-trainable params: 29,148,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the GRU+CNN model\n",
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word), dtype='int32', name='input_data')\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "gru_layer = GRU(128, activation = 'tanh',dropout=0.2, recurrent_dropout=0.2)#when return_sequences=true,the lstm model returns the outputs of every timestep\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "gru_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    sent_gru = gru_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    gru_out.append(sent_gru)\n",
    "gru_out = stack_layer(gru_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "#\n",
    "#处理句子，从头到尾\n",
    "sent_cnn = cnn_layer(gru_out)\n",
    "cnn_out = max_pooling_layer(sent_cnn)\n",
    "dense1 = Dense(50,activation='sigmoid')(cnn_out)\n",
    "dense2 = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[dense2])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "  800/25000 [..............................] - ETA: 5:10:41 - loss: 0.7318 - acc: 0.5113"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:18: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=100, activation=\"tanh\", padding=\"same\", kernel_size=3, strides=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 36, 224)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 224)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 224, 300)     30089100    lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 224, 100)     90100       embedding_6[0][0]                \n",
      "                                                                 embedding_6[1][0]                \n",
      "                                                                 embedding_6[2][0]                \n",
      "                                                                 embedding_6[3][0]                \n",
      "                                                                 embedding_6[4][0]                \n",
      "                                                                 embedding_6[5][0]                \n",
      "                                                                 embedding_6[6][0]                \n",
      "                                                                 embedding_6[7][0]                \n",
      "                                                                 embedding_6[8][0]                \n",
      "                                                                 embedding_6[9][0]                \n",
      "                                                                 embedding_6[10][0]               \n",
      "                                                                 embedding_6[11][0]               \n",
      "                                                                 embedding_6[12][0]               \n",
      "                                                                 embedding_6[13][0]               \n",
      "                                                                 embedding_6[14][0]               \n",
      "                                                                 embedding_6[15][0]               \n",
      "                                                                 embedding_6[16][0]               \n",
      "                                                                 embedding_6[17][0]               \n",
      "                                                                 embedding_6[18][0]               \n",
      "                                                                 embedding_6[19][0]               \n",
      "                                                                 embedding_6[20][0]               \n",
      "                                                                 embedding_6[21][0]               \n",
      "                                                                 embedding_6[22][0]               \n",
      "                                                                 embedding_6[23][0]               \n",
      "                                                                 embedding_6[24][0]               \n",
      "                                                                 embedding_6[25][0]               \n",
      "                                                                 embedding_6[26][0]               \n",
      "                                                                 embedding_6[27][0]               \n",
      "                                                                 embedding_6[28][0]               \n",
      "                                                                 embedding_6[29][0]               \n",
      "                                                                 embedding_6[30][0]               \n",
      "                                                                 embedding_6[31][0]               \n",
      "                                                                 embedding_6[32][0]               \n",
      "                                                                 embedding_6[33][0]               \n",
      "                                                                 embedding_6[34][0]               \n",
      "                                                                 embedding_6[35][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "                                                                 conv1d_4[1][0]                   \n",
      "                                                                 conv1d_4[2][0]                   \n",
      "                                                                 conv1d_4[3][0]                   \n",
      "                                                                 conv1d_4[4][0]                   \n",
      "                                                                 conv1d_4[5][0]                   \n",
      "                                                                 conv1d_4[6][0]                   \n",
      "                                                                 conv1d_4[7][0]                   \n",
      "                                                                 conv1d_4[8][0]                   \n",
      "                                                                 conv1d_4[9][0]                   \n",
      "                                                                 conv1d_4[10][0]                  \n",
      "                                                                 conv1d_4[11][0]                  \n",
      "                                                                 conv1d_4[12][0]                  \n",
      "                                                                 conv1d_4[13][0]                  \n",
      "                                                                 conv1d_4[14][0]                  \n",
      "                                                                 conv1d_4[15][0]                  \n",
      "                                                                 conv1d_4[16][0]                  \n",
      "                                                                 conv1d_4[17][0]                  \n",
      "                                                                 conv1d_4[18][0]                  \n",
      "                                                                 conv1d_4[19][0]                  \n",
      "                                                                 conv1d_4[20][0]                  \n",
      "                                                                 conv1d_4[21][0]                  \n",
      "                                                                 conv1d_4[22][0]                  \n",
      "                                                                 conv1d_4[23][0]                  \n",
      "                                                                 conv1d_4[24][0]                  \n",
      "                                                                 conv1d_4[25][0]                  \n",
      "                                                                 conv1d_4[26][0]                  \n",
      "                                                                 conv1d_4[27][0]                  \n",
      "                                                                 conv1d_4[28][0]                  \n",
      "                                                                 conv1d_4[29][0]                  \n",
      "                                                                 conv1d_4[30][0]                  \n",
      "                                                                 conv1d_4[31][0]                  \n",
      "                                                                 conv1d_4[32][0]                  \n",
      "                                                                 conv1d_4[33][0]                  \n",
      "                                                                 conv1d_4[34][0]                  \n",
      "                                                                 conv1d_4[35][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 36, 100)      0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_4[1][0]     \n",
      "                                                                 global_max_pooling1d_4[2][0]     \n",
      "                                                                 global_max_pooling1d_4[3][0]     \n",
      "                                                                 global_max_pooling1d_4[4][0]     \n",
      "                                                                 global_max_pooling1d_4[5][0]     \n",
      "                                                                 global_max_pooling1d_4[6][0]     \n",
      "                                                                 global_max_pooling1d_4[7][0]     \n",
      "                                                                 global_max_pooling1d_4[8][0]     \n",
      "                                                                 global_max_pooling1d_4[9][0]     \n",
      "                                                                 global_max_pooling1d_4[10][0]    \n",
      "                                                                 global_max_pooling1d_4[11][0]    \n",
      "                                                                 global_max_pooling1d_4[12][0]    \n",
      "                                                                 global_max_pooling1d_4[13][0]    \n",
      "                                                                 global_max_pooling1d_4[14][0]    \n",
      "                                                                 global_max_pooling1d_4[15][0]    \n",
      "                                                                 global_max_pooling1d_4[16][0]    \n",
      "                                                                 global_max_pooling1d_4[17][0]    \n",
      "                                                                 global_max_pooling1d_4[18][0]    \n",
      "                                                                 global_max_pooling1d_4[19][0]    \n",
      "                                                                 global_max_pooling1d_4[20][0]    \n",
      "                                                                 global_max_pooling1d_4[21][0]    \n",
      "                                                                 global_max_pooling1d_4[22][0]    \n",
      "                                                                 global_max_pooling1d_4[23][0]    \n",
      "                                                                 global_max_pooling1d_4[24][0]    \n",
      "                                                                 global_max_pooling1d_4[25][0]    \n",
      "                                                                 global_max_pooling1d_4[26][0]    \n",
      "                                                                 global_max_pooling1d_4[27][0]    \n",
      "                                                                 global_max_pooling1d_4[28][0]    \n",
      "                                                                 global_max_pooling1d_4[29][0]    \n",
      "                                                                 global_max_pooling1d_4[30][0]    \n",
      "                                                                 global_max_pooling1d_4[31][0]    \n",
      "                                                                 global_max_pooling1d_4[32][0]    \n",
      "                                                                 global_max_pooling1d_4[33][0]    \n",
      "                                                                 global_max_pooling1d_4[34][0]    \n",
      "                                                                 global_max_pooling1d_4[35][0]    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 36, 128)      87936       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          197376      gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           16448       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            65          dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,481,025\n",
      "Trainable params: 391,925\n",
      "Non-trainable params: 30,089,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the CNN+GRU model\n",
    "Bidirectional = keras.layers.Bidirectional\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word))\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "cnn_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    \n",
    "    sent_cnn = cnn_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn = max_pooling_layer(sent_cnn)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    cnn_out.append(sent_cnn)\n",
    "cnn_out = stack_layer(cnn_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "\n",
    "#处理句子，从头到尾\n",
    "hidden_layer_one = GRU(128, activation='tanh',dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(cnn_out)\n",
    "hidden_layer_two = Bidirectional(GRU(128, activation='tanh',dropout=0.2, recurrent_dropout=0.2))(hidden_layer_one)\n",
    "dense1 = Dense(64, activation='sigmoid')(hidden_layer_two)\n",
    "dense2 = Dense(1,activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[dense2])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "17500/25000 [====================>.........] - ETA: 21:35 - loss: 0.4665 - acc: 0.7631"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "batch_size = 100\n",
    "epoch_num = 5\n",
    "\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch_num,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paper\n",
    "* 单纯用cnn只能抓到n gram的信息，没办法拿到句子和句子的semantic relation\n",
    "* 单纯用rnn同理，rnn 处理long term dependency有局限，导致前面的内容没办法没model所抓取。\n",
    "* GRU>LSTM，后面的组合用gru\n",
    "* 不分句没办法抓取semantic relation,reference document paper,记录为什么要分句（层级word(hieratical)）\n",
    "* cnn+gru(cnn先抓取句子中重要的sentiment feature,rnn抓取每个句子之间的关系，最后输出feature vector)\n",
    "* gru+cnn(gru先抓取句子中的词语之间的序列关系，cnn再抓取重要的句子(gru_output)，但是不能拿到句子和句子之间的semantic relation，结果较差)\n",
    "* Kcnn+gru(加入eternel knowledge(extracting sentiword from SentiWordNet)，为了提升cnn+gru中cnn抓取sentiment word的能力，)，reference 老师paper\n",
    "* cnn+gru单纯based on data，所以model需要大量的high quality labelled data去学习（局限），会出现overfitting(training data少，或者质量不好)\n",
    "* KCNN+gru加入human knowledge，construct sentiment word filters for cnn，更有效地提取sentiment Word,minimize reliance on training data\n",
    "* rule-based的方法很难建立，其次在改的时候也需要改一连串的规则\n",
    "* DL语言的复杂性造成data很多，很难获取\n",
    "* DL的overfitting\n",
    "* NLP tookits, POS tagger(词性)\n",
    "* 实验调整filter 10~100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to write paper\n",
    "* 阅读paper\n",
    "* abstract+conclusion\n",
    "* 归类（based on data(传统ML for sentiment analysis,DL_NN for sentiment analysis,cnn/rnn/combination of cnn+rnn）,+externel knowledge,sentimentWordNet)),记录每个method的局限。ML 需要复杂的feature engineering，需要externel NLP toolkits（不完美）,会出现error，影响model，DL 过于依赖大量label data，记录externel knowledge method如何使用sentiwordnet,大部分可能是使用pattern matching,our model 优势把sentiment words 融入cnn中，instead of hard mapping words.word filter* word,和word_embedding 相似，输入越大，除了能抓取sentiment word net里面的senti words,还能抓到和sentiment word相似的senti words.因为filter 基于Word embedding，做convolution的时候拿到sentiment words和input words,similarity\n",
    "* reference 对应会议 reference style\n",
    "* 记录paper怎么做的，和limitation，和我们怎么改变的？\n",
    "* kcnn  filter parameter 少，random 的少"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
