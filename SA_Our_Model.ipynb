{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "* 获取senti words\n",
    "* 别忘了clean里面的符号（用clean代替一些代码）\n",
    "* 拿了大概五百个\n",
    "* filter weights用sent2net里的词汇\n",
    "* 从sentiwordnet,如何从文件里抓取极性词，分数大于0.8，建立两个得到list的情感词。分开positive和negative\n",
    "* 加到一个list\n",
    "* layer = return true\n",
    "\n",
    "* 自己得到cnn_filter weights\n",
    "* 默认train的时候得到weights是initilizer randomly的\n",
    "* 自己train的时候，trainable为false，因为已经得到情感词\n",
    "* weight.shape（window_size,feature_dimension(embedding),number of words/number of filter）\n",
    "* bias.shape(number of sentiment words/number of filter)一维list \n",
    "* 两个合在一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "* function:get the sentiment words,use the filters to get\n",
    "* input:()\n",
    "* filter:to extract some features\n",
    "* filter_length:the quantity of filters\n",
    "* max_pooling:\n",
    "* output:(batch_size*)\n",
    "* 用两个CNN，一个是knowledge的，如果没找完，就要用random了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "* function:\n",
    "* input:\n",
    "* output:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questions\n",
    "* 为什么maxpooling得到这个\n",
    "* 可以试试别的\n",
    "* filter的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "* 两个最重要的是vocab2int 和 embedding matrix\n",
    "* 纯CNN跑，用全部的数据，不用分句\n",
    "* 后面三个是以句子为单位\n",
    "* cnn layer2是基于知识的\n",
    "* 用skLearn得到recall 和precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import gensim\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 36, 224) (25000, 36, 224)\n",
      "embedding_matrix shape: (97162, 300)\n",
      "vocabulary size: 97162\n",
      "max sent length: 36 \n",
      "max word length: 224\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "maxlen_sent = train_copus_padded.shape[1]\n",
    "maxlen_word = train_copus_padded.shape[2]\n",
    "print('max sent length:', maxlen_sent, '\\nmax word length:', maxlen_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sentiment words from SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'resource/SentiWordNet_3.0.0_20130122.txt'\n",
    "content = []\n",
    "with open(file_path,'r',encoding='UTF-8') as f:\n",
    "    while(1):\n",
    "        lines = f.readline()\n",
    "        if(lines[0] is ('a' or 'n' or 'v' or 'r')):\n",
    "            content.append(lines.split('\\t'))\n",
    "        elif(lines[0] is '#'):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "             \n",
    "#get the sentiment words(score>0.8)\n",
    "neg_words1 = []\n",
    "pos_words1 = []\n",
    "for lines in content:\n",
    "    if(float(lines[2])>=0.8):\n",
    "        pos_words1.append(lines[4])\n",
    "    elif(float(lines[3])>=0.8):\n",
    "        neg_words1.append(lines[4])\n",
    "\n",
    "#get the positive sentiment words\n",
    "#seperate the multiple words\n",
    "pos_words2 = []\n",
    "for i in range(len(pos_words1)):\n",
    "    pos_words2.append(pos_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "pos_words3 = []\n",
    "for i in pos_words2:\n",
    "    for j in i:\n",
    "        pos_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "pos_words4 = []\n",
    "for elem in pos_words3:\n",
    "    pos_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "pos_lexicon = []\n",
    "for elem in pos_words4:\n",
    "    pos_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "    \n",
    "#get the negative sentiment words\n",
    "#seperate the multiple words\n",
    "neg_words2 = []\n",
    "for i in range(len(neg_words1)):\n",
    "    neg_words2.append(neg_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "neg_words3 = []\n",
    "for i in neg_words2:\n",
    "    for j in i:\n",
    "        neg_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "neg_words4 = []\n",
    "for elem in neg_words3:\n",
    "    neg_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "neg_lexicon = []\n",
    "for elem in neg_words4:\n",
    "    neg_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "#merge the positive and negative words\n",
    "senti_lexicon = pos_lexicon + neg_lexicon\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "senti_lexicon = shuffle(senti_lexicon,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map the sentiment words to integer based on vocab2int\n",
    "senti2int = [getWordIdx(word, vocab_to_int_train) for word in senti_lexicon]\n",
    "\n",
    "#get the filter weights based on the sentiment words&vocab2int&embedding_matrix\n",
    "def Find_Filter_Weight(senti2int):\n",
    "    \"\"\"sentiwords is the list\"\"\"\n",
    "    word_filter_weights = []\n",
    "    bias_weights = []\n",
    "    filter_len = 1\n",
    "    for i in senti2int:\n",
    "        vector = embedding_matrix[i]  # shape: 300\n",
    "        vector = np.expand_dims(vector, axis=0) #shape: 1x 300\n",
    "        vector = np.expand_dims(vector, axis=2) #shape: 1x 300 x 1\n",
    "        if len(word_filter_weights) == 0:\n",
    "            word_filter_weights = vector\n",
    "        else:\n",
    "            word_filter_weights = np.concatenate((word_filter_weights, vector), axis=2)   \n",
    "    #shape is (1, 300, 533)\n",
    "    \n",
    "    bias_weights = np.zeros(len(senti2int))\n",
    "    cnn_wordfilter_weights = [word_filter_weights, bias_weights]\n",
    "    \n",
    "    return cnn_wordfilter_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_weights = Find_Filter_Weight(senti2int)\n",
    "CNN_weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment \n",
    "* record the result of running the different vesions\n",
    "* change the parameter to run and record the result again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data prepocessing for CNN/GRU/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('data/aclImdb/train-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "test_data , test_label = shuffle(test_data ,test_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data for the cnn layer\n",
    "#word tokenizing the data\n",
    "training_data = []\n",
    "for data in train_data:\n",
    "    training_data.append(word_tokenize(data))\n",
    "\n",
    "testing_data = []\n",
    "for data in test_data:\n",
    "    testing_data.append(word_tokenize(data))\n",
    "    \n",
    "#map the word to integer\n",
    "x_train = []\n",
    "word2int = []\n",
    "for review in training_data:\n",
    "    word2int =[getWordIdx(word, vocab_to_int_train) for word in review]\n",
    "    x_train.append(word2int)\n",
    "    \n",
    "x_test = []\n",
    "word2int = []\n",
    "for review in testing_data:\n",
    "    word2int = [getWordIdx(word, vocab_to_int_train) for word in review]\n",
    "    x_test.append(word2int)\n",
    "\n",
    "\n",
    "#show and padding the data\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "sequence = keras.preprocessing.sequence\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_word)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_word)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "num_filter = 200\n",
    "dim = int(num_filter/2)\n",
    "batch_size = 300\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = Convolution1D(nb_filter=num_filter,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1\n",
    "                            )(embeded)\n",
    "cnn_out = GlobalMaxPooling1D()(hidden_layer)\n",
    "dense = Dense(dim, activation='sigmoid')(cnn_out)\n",
    "#dense = Dense(dim, activation='softmax')(cnn_out)\n",
    "Final = Dense(1,activation = 'sigmoid')(dense)#为什么换成softmax就很差\n",
    "model = Model(inputs=input_data, outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)\n",
    "#the validate is too much?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classical LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build LSTM model\n",
    "batch_size = 200\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "# hidden_layer = LSTM(128, activation = 'sigmoid',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "#hidden_layer = LSTM(128, activation = 'relu',dropout=0.2, recurrent_dropout=0.2)(embeded)\n",
    "# hidden_layer = LSTM(128, activation = 'tanh',dropout=0.1, recurrent_dropout=0.2)(embeded)\n",
    "hidden_layer = LSTM(128, activation = 'relu',dropout=0.1, recurrent_dropout=0.2)(embeded)\n",
    "dense = Dense(64,activation = 'relu')(hidden_layer)\n",
    "Final = Dense(1,activation = 'relu')(dense)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "\n",
    "          \n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    \n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classical GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build GRU model\n",
    "batch_size = 300\n",
    "num_epoch = 5\n",
    "\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = GRU(128, activation = 'tanh',dropout=0.1, recurrent_dropout=0.2)(embeded)\n",
    "dense = Dense(64,activation = 'relu')(hidden_layer)\n",
    "Final = Dense(1,activation = 'sigmoid')(dense)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "print('Train...')\n",
    "model.fit(x_train, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(x_test, test_label))\n",
    "score, acc = model.evaluate(x_test, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCNN+GRU(The document is seperated by sentence first and then by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the kCNN+GRU model\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word))\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "cnn_layer1 = Convolution1D(nb_filter=200,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='relu',\n",
    "                           weights = CNN_weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "cnn_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    \n",
    "    sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "    \n",
    "    cnn_out.append(sent_cnn)\n",
    "cnn_out = stack_layer(cnn_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "\n",
    "#处理句子，从头到尾\n",
    "gru= GRU(256, dropout=0.3, recurrent_dropout=0.2)(cnn_out)\n",
    "dense1 = Dense(128, activation='sigmoid')(gru)\n",
    "Final = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "batch_size = 300\n",
    "num_epoch = 5\n",
    "\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### GRU+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 36, 224)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 224)          0           input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 224, 300)     29148600    lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 128)          164736      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "                                                                 embedding_1[3][0]                \n",
      "                                                                 embedding_1[4][0]                \n",
      "                                                                 embedding_1[5][0]                \n",
      "                                                                 embedding_1[6][0]                \n",
      "                                                                 embedding_1[7][0]                \n",
      "                                                                 embedding_1[8][0]                \n",
      "                                                                 embedding_1[9][0]                \n",
      "                                                                 embedding_1[10][0]               \n",
      "                                                                 embedding_1[11][0]               \n",
      "                                                                 embedding_1[12][0]               \n",
      "                                                                 embedding_1[13][0]               \n",
      "                                                                 embedding_1[14][0]               \n",
      "                                                                 embedding_1[15][0]               \n",
      "                                                                 embedding_1[16][0]               \n",
      "                                                                 embedding_1[17][0]               \n",
      "                                                                 embedding_1[18][0]               \n",
      "                                                                 embedding_1[19][0]               \n",
      "                                                                 embedding_1[20][0]               \n",
      "                                                                 embedding_1[21][0]               \n",
      "                                                                 embedding_1[22][0]               \n",
      "                                                                 embedding_1[23][0]               \n",
      "                                                                 embedding_1[24][0]               \n",
      "                                                                 embedding_1[25][0]               \n",
      "                                                                 embedding_1[26][0]               \n",
      "                                                                 embedding_1[27][0]               \n",
      "                                                                 embedding_1[28][0]               \n",
      "                                                                 embedding_1[29][0]               \n",
      "                                                                 embedding_1[30][0]               \n",
      "                                                                 embedding_1[31][0]               \n",
      "                                                                 embedding_1[32][0]               \n",
      "                                                                 embedding_1[33][0]               \n",
      "                                                                 embedding_1[34][0]               \n",
      "                                                                 embedding_1[35][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 36, 128)      0           gru_1[0][0]                      \n",
      "                                                                 gru_1[1][0]                      \n",
      "                                                                 gru_1[2][0]                      \n",
      "                                                                 gru_1[3][0]                      \n",
      "                                                                 gru_1[4][0]                      \n",
      "                                                                 gru_1[5][0]                      \n",
      "                                                                 gru_1[6][0]                      \n",
      "                                                                 gru_1[7][0]                      \n",
      "                                                                 gru_1[8][0]                      \n",
      "                                                                 gru_1[9][0]                      \n",
      "                                                                 gru_1[10][0]                     \n",
      "                                                                 gru_1[11][0]                     \n",
      "                                                                 gru_1[12][0]                     \n",
      "                                                                 gru_1[13][0]                     \n",
      "                                                                 gru_1[14][0]                     \n",
      "                                                                 gru_1[15][0]                     \n",
      "                                                                 gru_1[16][0]                     \n",
      "                                                                 gru_1[17][0]                     \n",
      "                                                                 gru_1[18][0]                     \n",
      "                                                                 gru_1[19][0]                     \n",
      "                                                                 gru_1[20][0]                     \n",
      "                                                                 gru_1[21][0]                     \n",
      "                                                                 gru_1[22][0]                     \n",
      "                                                                 gru_1[23][0]                     \n",
      "                                                                 gru_1[24][0]                     \n",
      "                                                                 gru_1[25][0]                     \n",
      "                                                                 gru_1[26][0]                     \n",
      "                                                                 gru_1[27][0]                     \n",
      "                                                                 gru_1[28][0]                     \n",
      "                                                                 gru_1[29][0]                     \n",
      "                                                                 gru_1[30][0]                     \n",
      "                                                                 gru_1[31][0]                     \n",
      "                                                                 gru_1[32][0]                     \n",
      "                                                                 gru_1[33][0]                     \n",
      "                                                                 gru_1[34][0]                     \n",
      "                                                                 gru_1[35][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 36, 100)      38500       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           5050        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            51          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 29,356,937\n",
      "Trainable params: 208,337\n",
      "Non-trainable params: 29,148,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build the GRU+CNN model\n",
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word), dtype='int32', name='input_data')\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "gru_layer = GRU(128, activation = 'tanh',dropout=0.2, recurrent_dropout=0.2)#when return_sequences=true,the lstm model returns the outputs of every timestep\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "gru_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    sent_gru = gru_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    gru_out.append(sent_gru)\n",
    "gru_out = stack_layer(gru_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "#\n",
    "#处理句子，从头到尾\n",
    "sent_cnn = cnn_layer(gru_out)\n",
    "cnn_out = max_pooling_layer(sent_cnn)\n",
    "dense1 = Dense(50,activation='sigmoid')(cnn_out)\n",
    "dense2 = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[dense2])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "  800/25000 [..............................] - ETA: 5:10:41 - loss: 0.7318 - acc: 0.5113"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(x_test, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the CNN+GRU model\n",
    "def slice(x, index):\n",
    "    \"\"\" Define a tensor slice function\n",
    "    \n",
    "    \"\"\"\n",
    "    return x[:, index, :]\n",
    "\n",
    "\n",
    "\n",
    "input_data = Input(shape = (maxlen_sent,maxlen_word))\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "#************\n",
    "stack_layer = Lambda(lambda x: K.stack(x, axis=1))\n",
    "\n",
    "# interate through sentences in a document\n",
    "cnn_out = []\n",
    "for i in range(maxlen_sent):\n",
    "    #以每个影评的每个句子为输入\n",
    "    sent = Lambda(slice, arguments={'index': i,})(input_data)#use the lambda to enclose the function as the slicing layer\n",
    "    sent_embedding = embedding_layer(sent)#input shape:(padded_sentence_number),output shape:(nb_words_padded,dimension)\n",
    "    \n",
    "    sent_cnn = cnn_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "    # we use standard max over time pooling\n",
    "    sent_cnn = max_pooling_layer(sent_cnn)  # output shape: (None, nb_filter)\n",
    "    \n",
    "    cnn_out.append(sent_cnn)\n",
    "cnn_out = stack_layer(cnn_out)  # out shape: (None, maxlen_sent, nb_filter)\n",
    "\n",
    "\n",
    "\n",
    "#处理句子，从头到尾\n",
    "gru= GRU(128, dropout=0.2, recurrent_dropout=0.2)(cnn_out)\n",
    "dense1 = Dense(64, activation='sigmoid')(gru)\n",
    "dense2 = Dense(1,activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[dense2])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "batch_size = 200\n",
    "epoch_num = 5\n",
    "\n",
    "print('Training...')\n",
    "model.fit(train_copus_padded, train_label,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epoch_num,\n",
    "          validation_data=(test_copus_padded, test_label))\n",
    "\n",
    "score, acc = model.evaluate(test_copus_padded, test_label,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure the model\n",
    "pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "# predict the class label\n",
    "if pred_test_prob.shape[-1]>1:\n",
    "    pred_test = pred_test_prob.argmax(axis=-1)\n",
    "else:\n",
    "    pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "    pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "print(\"Accuracy: %.4f\" % (acc))   \n",
    "\n",
    "print(classification_report(test_label, pred_test, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paper\n",
    "* 单纯用cnn只能抓到n gram的信息，没办法拿到句子和句子的semantic relation\n",
    "* 单纯用rnn同理，rnn 处理long term dependency有局限，导致前面的内容没办法没model所抓取。\n",
    "* GRU>LSTM，后面的组合用gru\n",
    "* 不分句没办法抓取semantic relation,reference document paper,记录为什么要分句（层级word(hieratical)）\n",
    "* cnn+gru(cnn先抓取句子中重要的sentiment feature,rnn抓取每个句子之间的关系，最后输出feature vector)\n",
    "* gru+cnn(gru先抓取句子中的词语之间的序列关系，cnn再抓取重要的句子(gru_output)，但是不能拿到句子和句子之间的semantic relation，结果较差)\n",
    "* Kcnn+gru(加入eternel knowledge(extracting sentiword from SentiWordNet)，为了提升cnn+gru中cnn抓取sentiment word的能力，)，reference 老师paper\n",
    "* cnn+gru单纯based on data，所以model需要大量的high quality labelled data去学习（局限），会出现overfitting(training data少，或者质量不好)\n",
    "* KCNN+gru加入human knowledge，construct sentiment word filters for cnn，更有效地提取sentiment Word,minimize reliance on training data\n",
    "* rule-based的方法很难建立，其次在改的时候也需要改一连串的规则\n",
    "* DL语言的复杂性造成data很多，很难获取\n",
    "* DL的overfitting\n",
    "* NLP tookits, POS tagger(词性)\n",
    "* 实验调整filter 10~100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to write paper\n",
    "* 阅读paper\n",
    "* abstract+conclusion\n",
    "* 归类（based on data(传统ML for sentiment analysis,DL_NN for sentiment analysis,cnn/rnn/combination of cnn+rnn）,+externel knowledge,sentimentWordNet)),记录每个method的局限。ML 需要复杂的feature engineering，需要externel NLP toolkits（不完美）,会出现error，影响model，DL 过于依赖大量label data，记录externel knowledge method如何使用sentiwordnet,大部分可能是使用pattern matching,our model 优势把sentiment words 融入cnn中，instead of hard mapping words.word filter* word,和word_embedding 相似，输入越大，除了能抓取sentiment word net里面的senti words,还能抓到和sentiment word相似的senti words.因为filter 基于Word embedding，做convolution的时候拿到sentiment words和input words,similarity\n",
    "* reference 对应会议 reference style\n",
    "* 记录paper怎么做的，和limitation，和我们怎么改变的？\n",
    "* kcnn  filter parameter 少，random 的少"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
