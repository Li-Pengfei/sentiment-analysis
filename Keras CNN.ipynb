{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence\n",
    "* 获取数据\n",
    "* 搭建神经网络 sequential().add()\n",
    "* 编译神经网络 compile()\n",
    "* 训练神经网络 fit()\n",
    "* 评估神经网络 evalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unfamiliar concepts\n",
    "* filter: 相当于一个matrix，维度为（想要输出的维度*（词的维度*词的个数））\n",
    "* pool_size 池化窗口的大小，图 in brain\n",
    "* pooling: extracting some feature \n",
    "* local feature\n",
    "* CNN->抓取情感词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, Embedding,MaxPooling1D,Input\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "#to get the data,the number of the data is 25000\n",
    "#x_train中的是一组list，一个list是一个影评，里面的每个词是用它出现的频率从1-num_words，那这个list一得出来就是一组数\n",
    "#y_train中的是一组list，一个list代表的是这个影评的极性（0 or 1）\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:5000]\n",
    "y_train = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = x_test[:500]\n",
    "y_test = y_test[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 train sequences\n",
      "500 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (5000, 80)\n",
      "x_test shape: (500, 80)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#0做padding\n",
    "#maxlen根据文本来定\n",
    "#x_train is the list which is waiting for being cutted as the maxlen\n",
    "#maxlen is the maxim length of list\n",
    "#return a numpy matrix(length * maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 6s 1ms/step - loss: 0.6935 - acc: 0.4942 - val_loss: 0.6891 - val_acc: 0.5580\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 3s 592us/step - loss: 0.5563 - acc: 0.7500 - val_loss: 0.4753 - val_acc: 0.7700\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 3s 586us/step - loss: 0.2010 - acc: 0.9204 - val_loss: 0.4154 - val_acc: 0.8340\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 3s 586us/step - loss: 0.0364 - acc: 0.9946 - val_loss: 0.5151 - val_acc: 0.8260\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 3s 583us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5835 - val_acc: 0.8140\n",
      "500/500 [==============================] - 0s 143us/step\n",
      "Test accuracy: [0.5835485415458679, 0.8140000038146973]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#记录baseline algorithm\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "#follow the sequence to create the model\n",
    "\n",
    "model.add(Embedding(max_features, 32, input_length = maxlen))\n",
    "#max_feature->词汇表大小\n",
    "#I consider the max_features vocabulary\n",
    "#use 32 dimension to represent each word\n",
    "#the length of every review is maxlen\n",
    "\n",
    "model.add(Conv1D(filters = 64, kernel_size = 3, padding = 'same',activation = 'relu'))\n",
    "#************************对卷积层的维度不了解\n",
    "#filters: 整数，输出空间的维度 （即卷积中滤波器的输出数量）\n",
    "#kernel_size: 一个整数，或者单个整数表示的元组或列表， 指明 1D 卷积窗口的长度\n",
    "#\"same\" 表示填充输入以使输出的和原始输入的那个长度相等\n",
    "\n",
    "model.add(MaxPooling1D(pool_size = 2))#********global->number maxpooling->vector\n",
    "#Max pooling取每一个区域的最大值\n",
    "#pool_size: 整数，最大池化的窗口大小。\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation = 'relu'))\n",
    "model.add(Dense(125,activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))#dense + softmax\n",
    "#dense是创造一个全连接层，其参数为（输出数据的维度，输出数据的维度）\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss_and_metrics = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 80, 32)            640000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 80, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               640250    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 126       \n",
      "=================================================================\n",
      "Total params: 1,317,959\n",
      "Trainable params: 1,317,959\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model class used with functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_data = Input(shape = (maxlen,))\n",
    "embed = Embedding(max_features,50)(Input_data)\n",
    "hidden_layer = Conv1D(filters = 64,kernel_size = 3,padding = 'same',activation = 'relu')(embed)\n",
    "pooling = MaxPooling1D(pool_size = 2)(hidden_layer)\n",
    "#为什么需要在dense层之前flatten一下呢\n",
    "#Flatten_layer = Flatten()(pooling)\n",
    "#target = Dense(1,activation = 'sigmoid')(Flatten_layer)\n",
    "target = Dense(1,activation = 'sigmoid')(pooling)\n",
    "model = Model(inputs = Input_data,outputs = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15,  256,    4, ...,   19,  178,   32],\n",
       "       [ 125,   68,    2, ...,   16,  145,   95],\n",
       "       [ 645,  662,    8, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [ 123, 1424,   18, ...,    8,   30,   50],\n",
       "       [ 703,    5,   27, ...,    2, 3388, 1153],\n",
       "       [ 575,    8,   30, ...,  345, 1998,    2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_8 to have 3 dimensions, but got array with shape (5000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8f5cb02cc0a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     10\u001b[0m score, acc = model.evaluate(x_test, y_test,\n\u001b[0;32m     11\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_8 to have 3 dimensions, but got array with shape (5000, 1)"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
