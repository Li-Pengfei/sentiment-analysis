{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"         # 3 is can change to 0-3\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the train_copus_padded data from .pickle file\n",
    "file = open('pickle_data/train_copus_pad.pickle','rb')\n",
    "train_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_copus_pad.pickle','rb')\n",
    "test_copus_padded = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','rb')\n",
    "vocab_to_int_train = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/embedding_matrix','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/train_label.pickle','rb')\n",
    "train_label = pickle.load(file)\n",
    "\n",
    "file = open('pickle_data/test_label.pickle','rb')\n",
    "test_label = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test data shape: (25000, 36, 224) (25000, 36, 224)\n",
      "embedding_matrix shape: (97162, 300)\n",
      "vocabulary size: 97162\n",
      "max sent number in a review: 36 \n",
      "max words in a sentence: 224\n"
     ]
    }
   ],
   "source": [
    "print('train test data shape:',train_copus_padded.shape, test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int_train)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "MAX_SENTS = train_copus_padded.shape[1]\n",
    "MAX_SENT_LENGTH = train_copus_padded.shape[2]\n",
    "print('max sent number in a review:', MAX_SENTS, '\\nmax words in a sentence:', MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment word filter construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load senti_lexicon extracted from SentiWordNet\n",
    "file = open('pickle_data/senti_lexicon.pickle','rb')\n",
    "senti_lexicon = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map the sentiment words to integer based on vocab2int\n",
    "senti2int = [getWordIdx(word, vocab_to_int_train) for word in senti_lexicon if getWordIdx(word, vocab_to_int_train)!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the filter weights based on the sentiment words&vocab2int&embedding_matrix\n",
    "def Find_Filter_Weight(senti2int):\n",
    "    \"\"\"sentiwords is the list\"\"\"\n",
    "    word_filter_weights = []\n",
    "    bias_weights = []\n",
    "    filter_len = 1\n",
    "    for i in senti2int:\n",
    "        vector = embedding_matrix[i]  # shape: 300\n",
    "        vector = np.expand_dims(vector, axis=0) #shape: 1x 300\n",
    "        vector = np.expand_dims(vector, axis=2) #shape: 1x 300 x 1\n",
    "        if len(word_filter_weights) == 0:\n",
    "            word_filter_weights = vector\n",
    "        else:\n",
    "            word_filter_weights = np.concatenate((word_filter_weights, vector), axis=2)\n",
    "    #shape is (1, 300, 533)\n",
    "    \n",
    "    bias_weights = np.zeros(len(senti2int))\n",
    "    cnn_wordfilter_weights = [word_filter_weights, bias_weights]\n",
    "    \n",
    "    return cnn_wordfilter_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 410)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_weights = Find_Filter_Weight(senti2int)\n",
    "CNN_weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 50\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 50\n",
    "dense_dim = 40\n",
    "\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=50, kernel_size=3, strides=1, padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/admusr/anaconda2/envs/python3_pengfei/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", weights=[array([[[..., trainable=False, filters=410, kernel_size=1, strides=1, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=MAX_SENT_LENGTH, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer1 = Convolution1D(nb_filter=50,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                           weights = CNN_weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 224, 300)     29148600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 224, 50)      45050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 224, 410)     123410      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM multiple             0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 460)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_1[1][0]     \n",
      "==================================================================================================\n",
      "Total params: 29,317,060\n",
      "Trainable params: 45,050\n",
      "Non-trainable params: 29,272,010\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 36, 224)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 36, 460)           29317060  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 36, 100)           153300    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 29,479,582\n",
      "Trainable params: 207,572\n",
      "Non-trainable params: 29,272,010\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "rnn_out = rnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "# att_out = Dropout(dropout_rate)(att_out)\n",
    "\n",
    "dense = Dense(dense_dim, activation='tanh')(att_out)\n",
    "dense = Dropout(dropout_rate)(dense)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(dense) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(dense)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 58s 2ms/step - loss: 0.6864 - acc: 0.5328\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.8142\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7996    0.8385    0.8186     12500\n",
      "          1     0.8302    0.7898    0.8095     12500\n",
      "\n",
      "avg / total     0.8149    0.8142    0.8141     25000\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.3832 - acc: 0.8304\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.8363\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7690    0.9613    0.8545     12500\n",
      "          1     0.9484    0.7113    0.8129     12500\n",
      "\n",
      "avg / total     0.8587    0.8363    0.8337     25000\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.3032 - acc: 0.8738\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.8883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9164    0.8546    0.8844     12500\n",
      "          1     0.8638    0.9221    0.8920     12500\n",
      "\n",
      "avg / total     0.8901    0.8883    0.8882     25000\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.2733 - acc: 0.8891\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.8792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8316    0.9508    0.8872     12500\n",
      "          1     0.9426    0.8075    0.8698     12500\n",
      "\n",
      "avg / total     0.8871    0.8792    0.8785     25000\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.2470 - acc: 0.9008\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.9023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8976    0.9082    0.9029     12500\n",
      "          1     0.9071    0.8964    0.9017     12500\n",
      "\n",
      "avg / total     0.9023    0.9023    0.9023     25000\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.2245 - acc: 0.9119\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.9002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8734    0.9360    0.9036     12500\n",
      "          1     0.9311    0.8643    0.8964     12500\n",
      "\n",
      "avg / total     0.9022    0.9002    0.9000     25000\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.2070 - acc: 0.9188\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 32s 1ms/step\n",
      "Accuracy: 0.8940\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8560    0.9473    0.8993     12500\n",
      "          1     0.9410    0.8406    0.8880     12500\n",
      "\n",
      "avg / total     0.8985    0.8940    0.8937     25000\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.1914 - acc: 0.9288\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 32s 1ms/step\n",
      "Accuracy: 0.9064\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9004    0.9138    0.9071     12500\n",
      "          1     0.9125    0.8990    0.9057     12500\n",
      "\n",
      "avg / total     0.9065    0.9064    0.9064     25000\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.1718 - acc: 0.9371\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Accuracy: 0.9075\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9112    0.9030    0.9071     12500\n",
      "          1     0.9038    0.9120    0.9079     12500\n",
      "\n",
      "avg / total     0.9075    0.9075    0.9075     25000\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.1688 - acc: 0.9354\n",
      "Evaluating...\n",
      "25000/25000 [==============================] - 32s 1ms/step\n",
      "Accuracy: 0.9067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9242    0.8862    0.9048     12500\n",
      "          1     0.8907    0.9273    0.9086     12500\n",
      "\n",
      "avg / total     0.9074    0.9067    0.9067     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      " 3300/25000 [==>...........................] - ETA: 48s - loss: 0.1533 - acc: 0.9424"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
