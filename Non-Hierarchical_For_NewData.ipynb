{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\[\\].\\\",()!?;:/])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  \n",
    "    # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "path_pos = 'data/unprocessed/sorted_data/apparel/positive.review'\n",
    "path_neg = 'data/unprocessed/sorted_data/apparel/negative.review'\n",
    "\n",
    "#read the file\n",
    "file_pos = open(path_pos,'r',encoding='windows-1252')\n",
    "file_pos = file_pos.read()\n",
    "\n",
    "file_neg = open(path_neg,'r',encoding='windows-1252')\n",
    "file_neg = file_neg.read()\n",
    "\n",
    "#extact the file\n",
    "positive = BeautifulSoup(file_pos)\n",
    "positive = positive.find_all('review_text')#get all of the positive reviews\n",
    "for i in range(len(positive)):#convet the elements in postive to the string type\n",
    "    positive[i] = str(positive[i])\n",
    "\n",
    "negative = BeautifulSoup(file_neg)\n",
    "negative = negative.find_all('review_text')#get all the positive reviews\n",
    "for i in range(len(negative)):#convet the elements in negative to the string type\n",
    "    negative[i] = str(negative[i])\n",
    "    \n",
    "#eliminate the <review_text></review_text>tag in the reviews and normalize the text\n",
    "positive_doc = []\n",
    "for review in positive:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    review = review.lower()\n",
    "    positive_doc.append(review)\n",
    "\n",
    "negative_doc = []\n",
    "for review in negative:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    review = review.lower()\n",
    "    negative_doc.append(review)\n",
    "    \n",
    "#merge the data\n",
    "data = negative_doc + positive_doc\n",
    "\n",
    "#get the train label\n",
    "pos_label = [1 for i in range(len(positive_doc))]\n",
    "neg_label = [0 for i in range(len(negative_doc))]\n",
    "train_label = pos_label + neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading data\n",
    "file = open('pickle_New_Data/embedding_matrix.pickle','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "\n",
    "file = open('pickle_New_Data/vocab_to_int.pickle','rb')\n",
    "vocab_to_int = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the maxlen word\n",
    "maxlen_word = 0\n",
    "\n",
    "list_maxlen_word = []\n",
    "for i in range(len(data)):\n",
    "    list_maxlen_word.append((len(data[i])))\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word)\n",
    "maxlen_word = list_maxlen_word[int(len(list_maxlen_word1)*0.95)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map the word to integer\n",
    "data2int = []\n",
    "word2int = []\n",
    "for review in data:\n",
    "    word2int =[getWordIdx(word, vocab_to_int) for word in review]\n",
    "    data2int.append(word2int)\n",
    "\n",
    "#padding for data\n",
    "sequence = keras.preprocessing.sequence\n",
    "data_padded = sequence.pad_sequences(data2int, maxlen=maxlen_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the train label\n",
    "pos_label = [1 for i in range(len(positive_doc))]\n",
    "neg_label = [0 for i in range(len(negative_doc))]\n",
    "label = pos_label + neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data to train and test dataset\n",
    "train_data, test_data, train_label, test_label = train_test_split(\n",
    "data_padded, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1600, 123)\n",
      "test data shape: (400, 123)\n",
      "embedding_matrix shape: (8023, 300)\n",
      "vocabulary size: 8023\n"
     ]
    }
   ],
   "source": [
    "#the shape of the data\n",
    "print('train data shape:',train_data.shape)\n",
    "print('test data shape:',test_data.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int)\n",
    "print('vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # hyper-parameters\n",
    "# batch_size = 50\n",
    "# epoch_num = 15\n",
    "# drop_out = 0.2\n",
    "# gru_dim = 50\n",
    "# lstm_dim = 50\n",
    "# num_filter = 100\n",
    "# window_size = 3\n",
    "# atten_dim = 50\n",
    "\n",
    "# hyper-parameters hierarchical\n",
    "gru_dim = 128\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 50\n",
    "lstm_dim = 50\n",
    "\n",
    "batch_size = 50\n",
    "epoch_num = 15\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "    test_label_cat = np_utils.to_categorical(test_label)\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "    test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 123)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 123, 300)          2406900   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 123, 100)          90100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,497,202\n",
      "Trainable params: 90,302\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1\n",
    "                            )(embeded)\n",
    "cnn_out = GlobalMaxPooling1D()(hidden_layer)\n",
    "preds = Dense(2, activation='softmax')(cnn_out) # categorical output\n",
    "model = Model(input_data, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6730 - acc: 0.5706\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 573us/step\n",
      "Accuracy: 0.6800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6911    0.6567    0.6735       201\n",
      "           1     0.6699    0.7035    0.6863       199\n",
      "\n",
      "   micro avg     0.6800    0.6800    0.6800       400\n",
      "   macro avg     0.6805    0.6801    0.6799       400\n",
      "weighted avg     0.6805    0.6800    0.6798       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.6210 - acc: 0.6725\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 466us/step\n",
      "Accuracy: 0.6925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7708    0.5522    0.6435       201\n",
      "           1     0.6484    0.8342    0.7297       199\n",
      "\n",
      "   micro avg     0.6925    0.6925    0.6925       400\n",
      "   macro avg     0.7096    0.6932    0.6866       400\n",
      "weighted avg     0.7099    0.6925    0.6864       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5923 - acc: 0.7113\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 504us/step\n",
      "Accuracy: 0.6400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5941    0.8955    0.7143       201\n",
      "           1     0.7835    0.3819    0.5135       199\n",
      "\n",
      "   micro avg     0.6400    0.6400    0.6400       400\n",
      "   macro avg     0.6888    0.6387    0.6139       400\n",
      "weighted avg     0.6883    0.6400    0.6144       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5650 - acc: 0.7469\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 454us/step\n",
      "Accuracy: 0.7150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7122    0.7264    0.7192       201\n",
      "           1     0.7179    0.7035    0.7107       199\n",
      "\n",
      "   micro avg     0.7150    0.7150    0.7150       400\n",
      "   macro avg     0.7151    0.7149    0.7149       400\n",
      "weighted avg     0.7151    0.7150    0.7150       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5485 - acc: 0.7419\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 484us/step\n",
      "Accuracy: 0.6725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.8358    0.7195       201\n",
      "           1     0.7537    0.5075    0.6066       199\n",
      "\n",
      "   micro avg     0.6725    0.6725    0.6725       400\n",
      "   macro avg     0.6927    0.6717    0.6630       400\n",
      "weighted avg     0.6923    0.6725    0.6633       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.5215 - acc: 0.7825A: 0s - loss: 0.516\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 454us/step\n",
      "Accuracy: 0.7175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7157    0.7264    0.7210       201\n",
      "           1     0.7194    0.7085    0.7139       199\n",
      "\n",
      "   micro avg     0.7175    0.7175    0.7175       400\n",
      "   macro avg     0.7175    0.7175    0.7175       400\n",
      "weighted avg     0.7175    0.7175    0.7175       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4993 - acc: 0.7906\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 481us/step\n",
      "Accuracy: 0.6975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6835    0.7413    0.7112       201\n",
      "           1     0.7143    0.6533    0.6824       199\n",
      "\n",
      "   micro avg     0.6975    0.6975    0.6975       400\n",
      "   macro avg     0.6989    0.6973    0.6968       400\n",
      "weighted avg     0.6988    0.6975    0.6969       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4907 - acc: 0.7937\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 431us/step\n",
      "Accuracy: 0.7075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6858    0.7711    0.7260       201\n",
      "           1     0.7356    0.6432    0.6863       199\n",
      "\n",
      "   micro avg     0.7075    0.7075    0.7075       400\n",
      "   macro avg     0.7107    0.7072    0.7062       400\n",
      "weighted avg     0.7106    0.7075    0.7063       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4630 - acc: 0.8125\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 424us/step\n",
      "Accuracy: 0.6950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6513    0.8458    0.7359       201\n",
      "           1     0.7770    0.5427    0.6391       199\n",
      "\n",
      "   micro avg     0.6950    0.6950    0.6950       400\n",
      "   macro avg     0.7142    0.6942    0.6875       400\n",
      "weighted avg     0.7138    0.6950    0.6877       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4493 - acc: 0.8269\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 494us/step\n",
      "Accuracy: 0.7325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7527    0.6965    0.7235       201\n",
      "           1     0.7150    0.7688    0.7409       199\n",
      "\n",
      "   micro avg     0.7325    0.7325    0.7325       400\n",
      "   macro avg     0.7338    0.7327    0.7322       400\n",
      "weighted avg     0.7339    0.7325    0.7322       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4305 - acc: 0.8337\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 529us/step\n",
      "Accuracy: 0.7075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6680    0.8308    0.7406       201\n",
      "           1     0.7733    0.5829    0.6648       199\n",
      "\n",
      "   micro avg     0.7075    0.7075    0.7075       400\n",
      "   macro avg     0.7207    0.7069    0.7027       400\n",
      "weighted avg     0.7204    0.7075    0.7029       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4286 - acc: 0.8150\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 494us/step\n",
      "Accuracy: 0.6800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6254    0.9055    0.7398       201\n",
      "           1     0.8257    0.4523    0.5844       199\n",
      "\n",
      "   micro avg     0.6800    0.6800    0.6800       400\n",
      "   macro avg     0.7256    0.6789    0.6621       400\n",
      "weighted avg     0.7251    0.6800    0.6625       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4399 - acc: 0.8075\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 544us/step\n",
      "Accuracy: 0.6675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6104    0.9353    0.7387       201\n",
      "           1     0.8587    0.3970    0.5430       199\n",
      "\n",
      "   micro avg     0.6675    0.6675    0.6675       400\n",
      "   macro avg     0.7345    0.6662    0.6408       400\n",
      "weighted avg     0.7339    0.6675    0.6413       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.4073 - acc: 0.8319\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 519us/step\n",
      "Accuracy: 0.7475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7336    0.7811    0.7566       201\n",
      "           1     0.7634    0.7136    0.7377       199\n",
      "\n",
      "   micro avg     0.7475    0.7475    0.7475       400\n",
      "   macro avg     0.7485    0.7473    0.7471       400\n",
      "weighted avg     0.7485    0.7475    0.7472       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3862 - acc: 0.8562\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 501us/step\n",
      "Accuracy: 0.7500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7349    0.7861    0.7596       201\n",
      "           1     0.7676    0.7136    0.7396       199\n",
      "\n",
      "   micro avg     0.7500    0.7500    0.7500       400\n",
      "   macro avg     0.7512    0.7498    0.7496       400\n",
      "weighted avg     0.7511    0.7500    0.7496       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_data, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_data, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_data, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for epoch 15/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 2s 1ms/step - loss: 0.3862 - acc: 0.8562\n",
    "Evaluating...\n",
    "400/400 [==============================] - 0s 501us/step\n",
    "Accuracy: 0.7500\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.7349    0.7861    0.7596       201\n",
    "           1     0.7676    0.7136    0.7396       199\n",
    "\n",
    "   micro avg     0.7500    0.7500    0.7500       400\n",
    "   macro avg     0.7512    0.7498    0.7496       400\n",
    "weighted avg     0.7511    0.7500    0.7496       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 123)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 123, 300)          2406900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,477,202\n",
      "Trainable params: 70,302\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build LSTM model\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = LSTM(lstm_dim, activation = 'relu',dropout=dropout_rate, recurrent_dropout=dropout_rate)(embeded)\n",
    "Final = Dense(2,activation = 'softmax')(hidden_layer)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.6932 - acc: 0.5369\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.5050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.0199    0.0388       201\n",
      "           1     0.5013    0.9950    0.6667       199\n",
      "\n",
      "   micro avg     0.5050    0.5050    0.5050       400\n",
      "   macro avg     0.6506    0.5074    0.3528       400\n",
      "weighted avg     0.6514    0.5050    0.3512       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6886 - acc: 0.5450\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 736us/step\n",
      "Accuracy: 0.5100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5104    0.6119    0.5566       201\n",
      "           1     0.5094    0.4070    0.4525       199\n",
      "\n",
      "   micro avg     0.5100    0.5100    0.5100       400\n",
      "   macro avg     0.5099    0.5095    0.5045       400\n",
      "weighted avg     0.5099    0.5100    0.5048       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6877 - acc: 0.5344\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 858us/step\n",
      "Accuracy: 0.5200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5319    0.3731    0.4386       201\n",
      "           1     0.5135    0.6683    0.5808       199\n",
      "\n",
      "   micro avg     0.5200    0.5200    0.5200       400\n",
      "   macro avg     0.5227    0.5207    0.5097       400\n",
      "weighted avg     0.5228    0.5200    0.5093       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6865 - acc: 0.5563\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 731us/step\n",
      "Accuracy: 0.5350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5330    0.6020    0.5654       201\n",
      "           1     0.5376    0.4673    0.5000       199\n",
      "\n",
      "   micro avg     0.5350    0.5350    0.5350       400\n",
      "   macro avg     0.5353    0.5347    0.5327       400\n",
      "weighted avg     0.5353    0.5350    0.5329       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6848 - acc: 0.5588\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 753us/step\n",
      "Accuracy: 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5351    0.3035    0.3873       201\n",
      "           1     0.5105    0.7337    0.6021       199\n",
      "\n",
      "   micro avg     0.5175    0.5175    0.5175       400\n",
      "   macro avg     0.5228    0.5186    0.4947       400\n",
      "weighted avg     0.5229    0.5175    0.4941       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6797 - acc: 0.5700\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 765us/step\n",
      "Accuracy: 0.5375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5488    0.4478    0.4932       201\n",
      "           1     0.5297    0.6281    0.5747       199\n",
      "\n",
      "   micro avg     0.5375    0.5375    0.5375       400\n",
      "   macro avg     0.5392    0.5380    0.5339       400\n",
      "weighted avg     0.5393    0.5375    0.5337       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6792 - acc: 0.5706\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 905us/step\n",
      "Accuracy: 0.5400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5570    0.4129    0.4743       201\n",
      "           1     0.5299    0.6683    0.5911       199\n",
      "\n",
      "   micro avg     0.5400    0.5400    0.5400       400\n",
      "   macro avg     0.5435    0.5406    0.5327       400\n",
      "weighted avg     0.5435    0.5400    0.5324       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6819 - acc: 0.5700\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 900us/step\n",
      "Accuracy: 0.5225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5219    0.5920    0.5548       201\n",
      "           1     0.5233    0.4523    0.4852       199\n",
      "\n",
      "   micro avg     0.5225    0.5225    0.5225       400\n",
      "   macro avg     0.5226    0.5222    0.5200       400\n",
      "weighted avg     0.5226    0.5225    0.5202       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6755 - acc: 0.5925\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 803us/step\n",
      "Accuracy: 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5260    0.4030    0.4563       201\n",
      "           1     0.5122    0.6332    0.5663       199\n",
      "\n",
      "   micro avg     0.5175    0.5175    0.5175       400\n",
      "   macro avg     0.5191    0.5181    0.5113       400\n",
      "weighted avg     0.5191    0.5175    0.5110       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6816 - acc: 0.5725\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 888us/step\n",
      "Accuracy: 0.5375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5374    0.5721    0.5542       201\n",
      "           1     0.5376    0.5025    0.5195       199\n",
      "\n",
      "   micro avg     0.5375    0.5375    0.5375       400\n",
      "   macro avg     0.5375    0.5373    0.5368       400\n",
      "weighted avg     0.5375    0.5375    0.5369       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6713 - acc: 0.5825\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 830us/step\n",
      "Accuracy: 0.5200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.3134    0.3962       201\n",
      "           1     0.5124    0.7286    0.6017       199\n",
      "\n",
      "   micro avg     0.5200    0.5200    0.5200       400\n",
      "   macro avg     0.5254    0.5210    0.4989       400\n",
      "weighted avg     0.5255    0.5200    0.4984       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6718 - acc: 0.5750\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 855us/step\n",
      "Accuracy: 0.5400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5475    0.4876    0.5158       201\n",
      "           1     0.5339    0.5930    0.5619       199\n",
      "\n",
      "   micro avg     0.5400    0.5400    0.5400       400\n",
      "   macro avg     0.5407    0.5403    0.5388       400\n",
      "weighted avg     0.5407    0.5400    0.5387       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 3s 2ms/step - loss: 0.6659 - acc: 0.5862\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 825us/step\n",
      "Accuracy: 0.5200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5207    0.5622    0.5407       201\n",
      "           1     0.5191    0.4774    0.4974       199\n",
      "\n",
      "   micro avg     0.5200    0.5200    0.5200       400\n",
      "   macro avg     0.5199    0.5198    0.5190       400\n",
      "weighted avg     0.5199    0.5200    0.5191       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6648 - acc: 0.5969\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 878us/step\n",
      "Accuracy: 0.5375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5333    0.6368    0.5805       201\n",
      "           1     0.5437    0.4372    0.4847       199\n",
      "\n",
      "   micro avg     0.5375    0.5375    0.5375       400\n",
      "   macro avg     0.5385    0.5370    0.5326       400\n",
      "weighted avg     0.5385    0.5375    0.5328       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 4s 3ms/step - loss: 0.6619 - acc: 0.5994\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 845us/step\n",
      "Accuracy: 0.5200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5236    0.4975    0.5102       201\n",
      "           1     0.5167    0.5427    0.5294       199\n",
      "\n",
      "   micro avg     0.5200    0.5200    0.5200       400\n",
      "   macro avg     0.5202    0.5201    0.5198       400\n",
      "weighted avg     0.5202    0.5200    0.5198       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_data, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_data, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_data, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for epoch 12/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 4s 2ms/step - loss: 0.6718 - acc: 0.5750\n",
    "Evaluating...\n",
    "400/400 [==============================] - 0s 855us/step\n",
    "Accuracy: 0.5400\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5475    0.4876    0.5158       201\n",
    "           1     0.5339    0.5930    0.5619       199\n",
    "\n",
    "   micro avg     0.5400    0.5400    0.5400       400\n",
    "   macro avg     0.5407    0.5403    0.5388       400\n",
    "weighted avg     0.5407    0.5400    0.5387       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 123)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 123, 300)          2406900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               164736    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,571,894\n",
      "Trainable params: 164,994\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build GRU model\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = GRU(gru_dim, activation = 'relu',dropout=dropout_rate, recurrent_dropout=dropout_rate)(embeded)\n",
    "Final = Dense(2,activation = 'softmax')(hidden_layer)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.6926 - acc: 0.5250\n",
      "Evaluating...\n",
      "400/400 [==============================] - 4s 10ms/step\n",
      "Accuracy: 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5318    0.5821    0.5558       201\n",
      "           1     0.5333    0.4824    0.5066       199\n",
      "\n",
      "   micro avg     0.5325    0.5325    0.5325       400\n",
      "   macro avg     0.5326    0.5323    0.5312       400\n",
      "weighted avg     0.5326    0.5325    0.5313       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6857 - acc: 0.5681\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5398    0.3035    0.3885       201\n",
      "           1     0.5122    0.7387    0.6049       199\n",
      "\n",
      "   micro avg     0.5200    0.5200    0.5200       400\n",
      "   macro avg     0.5260    0.5211    0.4967       400\n",
      "weighted avg     0.5261    0.5200    0.4962       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 3ms/step - loss: 0.6861 - acc: 0.5519\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5433    0.5622    0.5526       201\n",
      "           1     0.5417    0.5226    0.5320       199\n",
      "\n",
      "   micro avg     0.5425    0.5425    0.5425       400\n",
      "   macro avg     0.5425    0.5424    0.5423       400\n",
      "weighted avg     0.5425    0.5425    0.5423       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6842 - acc: 0.5663\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5410    0.4925    0.5156       201\n",
      "           1     0.5300    0.5779    0.5529       199\n",
      "\n",
      "   micro avg     0.5350    0.5350    0.5350       400\n",
      "   macro avg     0.5355    0.5352    0.5343       400\n",
      "weighted avg     0.5355    0.5350    0.5342       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6790 - acc: 0.5650A: 3s - loss: \n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.5400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5395    0.5771    0.5577       201\n",
      "           1     0.5405    0.5025    0.5208       199\n",
      "\n",
      "   micro avg     0.5400    0.5400    0.5400       400\n",
      "   macro avg     0.5400    0.5398    0.5393       400\n",
      "weighted avg     0.5400    0.5400    0.5394       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6769 - acc: 0.5650\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5361    0.4428    0.4850       201\n",
      "           1     0.5214    0.6131    0.5635       199\n",
      "\n",
      "   micro avg     0.5275    0.5275    0.5275       400\n",
      "   macro avg     0.5288    0.5279    0.5243       400\n",
      "weighted avg     0.5288    0.5275    0.5241       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6834 - acc: 0.5556\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5278    0.5672    0.5468       201\n",
      "           1     0.5272    0.4874    0.5065       199\n",
      "\n",
      "   micro avg     0.5275    0.5275    0.5275       400\n",
      "   macro avg     0.5275    0.5273    0.5266       400\n",
      "weighted avg     0.5275    0.5275    0.5267       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6745 - acc: 0.5825\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5350    0.5323    0.5337       201\n",
      "           1     0.5300    0.5327    0.5313       199\n",
      "\n",
      "   micro avg     0.5325    0.5325    0.5325       400\n",
      "   macro avg     0.5325    0.5325    0.5325       400\n",
      "weighted avg     0.5325    0.5325    0.5325       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6729 - acc: 0.5769\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5463    0.6169    0.5794       201\n",
      "           1     0.5549    0.4824    0.5161       199\n",
      "\n",
      "   micro avg     0.5500    0.5500    0.5500       400\n",
      "   macro avg     0.5506    0.5497    0.5478       400\n",
      "weighted avg     0.5506    0.5500    0.5479       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6710 - acc: 0.5944\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.5475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5467    0.5821    0.5639       201\n",
      "           1     0.5484    0.5126    0.5299       199\n",
      "\n",
      "   micro avg     0.5475    0.5475    0.5475       400\n",
      "   macro avg     0.5476    0.5473    0.5469       400\n",
      "weighted avg     0.5476    0.5475    0.5469       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6641 - acc: 0.5913A: 4s - l\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5663    0.5522    0.5592       201\n",
      "           1     0.5588    0.5729    0.5658       199\n",
      "\n",
      "   micro avg     0.5625    0.5625    0.5625       400\n",
      "   macro avg     0.5626    0.5626    0.5625       400\n",
      "weighted avg     0.5626    0.5625    0.5625       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.6579 - acc: 0.6094\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5896    0.3930    0.4716       201\n",
      "           1     0.5414    0.7236    0.6194       199\n",
      "\n",
      "   micro avg     0.5575    0.5575    0.5575       400\n",
      "   macro avg     0.5655    0.5583    0.5455       400\n",
      "weighted avg     0.5656    0.5575    0.5451       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6597 - acc: 0.6044\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5556    0.5721    0.5637       201\n",
      "           1     0.5544    0.5377    0.5459       199\n",
      "\n",
      "   micro avg     0.5550    0.5550    0.5550       400\n",
      "   macro avg     0.5550    0.5549    0.5548       400\n",
      "weighted avg     0.5550    0.5550    0.5549       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.6466 - acc: 0.6219\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5529    0.4677    0.5067       201\n",
      "           1     0.5348    0.6181    0.5734       199\n",
      "\n",
      "   micro avg     0.5425    0.5425    0.5425       400\n",
      "   macro avg     0.5439    0.5429    0.5401       400\n",
      "weighted avg     0.5439    0.5425    0.5399       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6422 - acc: 0.6256\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.5775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5909    0.5174    0.5517       201\n",
      "           1     0.5670    0.6382    0.6005       199\n",
      "\n",
      "   micro avg     0.5775    0.5775    0.5775       400\n",
      "   macro avg     0.5789    0.5778    0.5761       400\n",
      "weighted avg     0.5790    0.5775    0.5760       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_data, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_data, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_data, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for epoch 15/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 6s 4ms/step - loss: 0.6422 - acc: 0.6256\n",
    "Evaluating...\n",
    "400/400 [==============================] - 1s 1ms/step\n",
    "Accuracy: 0.5775\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.5909    0.5174    0.5517       201\n",
    "           1     0.5670    0.6382    0.6005       199\n",
    "\n",
    "   micro avg     0.5775    0.5775    0.5775       400\n",
    "   macro avg     0.5789    0.5778    0.5761       400\n",
    "weighted avg     0.5790    0.5775    0.5760       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biGRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 123)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 123, 300)          2406900   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 123, 256)          329472    \n",
      "_________________________________________________________________\n",
      "att_layer_1 (AttLayer)       (None, 256)               12900     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,749,786\n",
      "Trainable params: 342,886\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build GRU model\n",
    "input_data = Input(shape=(maxlen_word,))\n",
    "embeded = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)(input_data)\n",
    "hidden_layer = Bidirectional(GRU(gru_dim, activation = 'relu',dropout=dropout_rate, recurrent_dropout=dropout_rate,return_sequences = True))(embeded)\n",
    "att_out = AttLayer(atten_dim)(hidden_layer)\n",
    "Final = Dense(2,activation = 'softmax')(att_out)\n",
    "\n",
    "#compile the model\n",
    "model = Model(inputs=[input_data], outputs=[Final])\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 29s 18ms/step - loss: 0.6953 - acc: 0.4988\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 8ms/step\n",
      "Accuracy: 0.5150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5089    1.0000    0.6745       201\n",
      "           1     1.0000    0.0251    0.0490       199\n",
      "\n",
      "   micro avg     0.5150    0.5150    0.5150       400\n",
      "   macro avg     0.7544    0.5126    0.3618       400\n",
      "weighted avg     0.7532    0.5150    0.3633       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 8ms/step - loss: 0.6935 - acc: 0.5156\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5552    0.7761    0.6473       201\n",
      "           1     0.6218    0.3719    0.4654       199\n",
      "\n",
      "   micro avg     0.5750    0.5750    0.5750       400\n",
      "   macro avg     0.5885    0.5740    0.5564       400\n",
      "weighted avg     0.5883    0.5750    0.5568       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.6896 - acc: 0.5469\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6491    0.1841    0.2868       201\n",
      "           1     0.5219    0.8995    0.6605       199\n",
      "\n",
      "   micro avg     0.5400    0.5400    0.5400       400\n",
      "   macro avg     0.5855    0.5418    0.4737       400\n",
      "weighted avg     0.5858    0.5400    0.4727       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.6877 - acc: 0.5456\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7778    0.0697    0.1279       201\n",
      "           1     0.5105    0.9799    0.6713       199\n",
      "\n",
      "   micro avg     0.5225    0.5225    0.5225       400\n",
      "   macro avg     0.6441    0.5248    0.3996       400\n",
      "weighted avg     0.6448    0.5225    0.3982       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.6903 - acc: 0.5481\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6491    0.3682    0.4698       201\n",
      "           1     0.5559    0.7990    0.6557       199\n",
      "\n",
      "   micro avg     0.5825    0.5825    0.5825       400\n",
      "   macro avg     0.6025    0.5836    0.5628       400\n",
      "weighted avg     0.6028    0.5825    0.5623       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6822 - acc: 0.5800\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5960    0.4478    0.5114       201\n",
      "           1     0.5542    0.6935    0.6161       199\n",
      "\n",
      "   micro avg     0.5700    0.5700    0.5700       400\n",
      "   macro avg     0.5751    0.5706    0.5637       400\n",
      "weighted avg     0.5752    0.5700    0.5635       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.6704 - acc: 0.5919\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5800\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6065    0.4677    0.5281       201\n",
      "           1     0.5633    0.6935    0.6216       199\n",
      "\n",
      "   micro avg     0.5800    0.5800    0.5800       400\n",
      "   macro avg     0.5849    0.5806    0.5749       400\n",
      "weighted avg     0.5850    0.5800    0.5746       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 15s 10ms/step - loss: 0.6527 - acc: 0.6144\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.5825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5966    0.5224    0.5570       201\n",
      "           1     0.5714    0.6432    0.6052       199\n",
      "\n",
      "   micro avg     0.5825    0.5825    0.5825       400\n",
      "   macro avg     0.5840    0.5828    0.5811       400\n",
      "weighted avg     0.5841    0.5825    0.5810       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6225 - acc: 0.6525\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6034    0.7114    0.6530       201\n",
      "           1     0.6442    0.5276    0.5801       199\n",
      "\n",
      "   micro avg     0.6200    0.6200    0.6200       400\n",
      "   macro avg     0.6238    0.6195    0.6165       400\n",
      "weighted avg     0.6237    0.6200    0.6167       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6439 - acc: 0.6644\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6188    0.5572    0.5864       201\n",
      "           1     0.5936    0.6533    0.6220       199\n",
      "\n",
      "   micro avg     0.6050    0.6050    0.6050       400\n",
      "   macro avg     0.6062    0.6052    0.6042       400\n",
      "weighted avg     0.6063    0.6050    0.6041       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5887 - acc: 0.6806\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6278    0.6965    0.6604       201\n",
      "           1     0.6554    0.5829    0.6170       199\n",
      "\n",
      "   micro avg     0.6400    0.6400    0.6400       400\n",
      "   macro avg     0.6416    0.6397    0.6387       400\n",
      "weighted avg     0.6415    0.6400    0.6388       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5879 - acc: 0.6950\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6740    0.6070    0.6387       201\n",
      "           1     0.6393    0.7035    0.6699       199\n",
      "\n",
      "   micro avg     0.6550    0.6550    0.6550       400\n",
      "   macro avg     0.6567    0.6552    0.6543       400\n",
      "weighted avg     0.6567    0.6550    0.6542       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5573 - acc: 0.7113\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.5224    0.6034       201\n",
      "           1     0.6206    0.7889    0.6947       199\n",
      "\n",
      "   micro avg     0.6550    0.6550    0.6550       400\n",
      "   macro avg     0.6674    0.6557    0.6491       400\n",
      "weighted avg     0.6677    0.6550    0.6488       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5405 - acc: 0.7250\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6606    0.7264    0.6919       201\n",
      "           1     0.6927    0.6231    0.6561       199\n",
      "\n",
      "   micro avg     0.6750    0.6750    0.6750       400\n",
      "   macro avg     0.6767    0.6747    0.6740       400\n",
      "weighted avg     0.6766    0.6750    0.6741       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5283 - acc: 0.7281\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.6925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6711    0.7612    0.7133       201\n",
      "           1     0.7209    0.6231    0.6685       199\n",
      "\n",
      "   micro avg     0.6925    0.6925    0.6925       400\n",
      "   macro avg     0.6960    0.6922    0.6909       400\n",
      "weighted avg     0.6959    0.6925    0.6910       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_data, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_data, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_data, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for epoch 15/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 14s 9ms/step - loss: 0.5283 - acc: 0.7281\n",
    "Evaluating...\n",
    "400/400 [==============================] - 1s 3ms/step\n",
    "Accuracy: 0.6925\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.6711    0.7612    0.7133       201\n",
    "           1     0.7209    0.6231    0.6685       199\n",
    "\n",
    "   micro avg     0.6925    0.6925    0.6925       400\n",
    "   macro avg     0.6960    0.6922    0.6909       400\n",
    "weighted avg     0.6959    0.6925    0.6910       400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
