{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "* 1.loading data\n",
    "* 2.tokenizing data to the list\n",
    "* 3.build the vacabulary to integer\n",
    "* 4.padding the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dirname = 'data/aclImdb'\n",
    "# filename = 'aclImdb_v1.tar.gz'\n",
    "# locale.setlocale(locale.LC_ALL, 'C')\n",
    "# all_lines = []\n",
    "\n",
    "# if sys.version > '3':\n",
    "#     control_chars = [chr(0x85)]\n",
    "# else:\n",
    "#     control_chars = [unichr(0x85)]\n",
    "\n",
    "# # Convert text to lower-case and strip punctuation/symbols from words\n",
    "# def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "#     # Replace breaks with spaces\n",
    "#     norm_text = norm_text.replace('<br />', ' ')\n",
    "#     # Pad punctuation with spaces on both sides\n",
    "#     norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "#     return norm_text\n",
    "\n",
    "# if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "#     if not os.path.isdir(dirname):\n",
    "#         if not os.path.isfile(filename):\n",
    "#             # Download IMDB archive\n",
    "#             print(\"Downloading IMDB archive...\")\n",
    "#             url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "#             r = requests.get(url)\n",
    "#             with smart_open(filename, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "#         tar = tarfile.open(filename, mode='r')\n",
    "#         tar.extractall()\n",
    "#         tar.close()\n",
    "#     else:\n",
    "#         print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "#     # Collect & normalize test/train data\n",
    "#     print(\"Cleaning up dataset...\")\n",
    "#     folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "#     for fol in folders:\n",
    "#         temp = u''\n",
    "#         newline = \"\\n\".encode(\"utf-8\")\n",
    "#         output = fol.replace('/', '-') + '.txt'\n",
    "#         # Is there a better pattern to use?\n",
    "#         txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "#         print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "#         with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "#             for i, txt in enumerate(txt_files):\n",
    "#                 with smart_open(txt, \"rb\") as t:\n",
    "#                     one_text = t.read().decode(\"utf-8\")\n",
    "#                     for c in control_chars:\n",
    "#                         one_text = one_text.replace(c, ' ')\n",
    "#                     one_text = normalize_text(one_text)\n",
    "#                     all_lines.append(one_text)\n",
    "#                     n.write(one_text.encode(\"utf-8\"))\n",
    "#                     n.write(newline)\n",
    "\n",
    "#     # Save to disk for instant re-use on any future runs\n",
    "#     with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "#         for idx, line in enumerate(all_lines):\n",
    "#             num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "#             f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "# assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "# print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix(embed_path, vocab2int, EMBEDDING_DIM, embed_type='glove'):\n",
    "    #**********embed_path是glove的URL吗\n",
    "    #**********EMBEDDING_DIM是什么\n",
    "    \"\"\"\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "    if embed_type == 'word2vec': \n",
    "        \"\"\"Loads 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\"\"\"\n",
    "        with open(embed_path, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            vocab_size, layer1_size = map(int, header.split())\n",
    "            binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "            for line in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == ' ':\n",
    "                        word = ''.join(word)\n",
    "                        break\n",
    "                    if ch != '\\n':\n",
    "                        word.append(ch)\n",
    "                word = word.decode('utf-8', 'ignore')\n",
    "                embeddings[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                continue\n",
    "    else:\n",
    "        # load Glove or Dependency-based word embeddings\n",
    "        f = open(embed_path, \"rb\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        f.close()\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('D:/code_stock/SA/data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('D:/code_stock/SA/data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('D:/code_stock/SA/data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('D:/code_stock/SA/data/aclImdb/train-pos.txt')\n",
    "\n",
    "# test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "# test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "# train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "# train_pos = readfile('data/aclImdb/train-pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "tese_data , test_label = shuffle(test_data ,train_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        train_data_word[i].append(word_tokenize(train_data_sent[i][j]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int = build_vocab(train_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the padding element\n",
    "maxlen_word = 0\n",
    "maxlen_sent = 0\n",
    "pad_train_set = []\n",
    "list_maxlen_sent = []\n",
    "list_maxlen_word = []\n",
    "\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent.append((len(train_data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word.append(len(train_data_sent[i][j]))\n",
    "\n",
    "#get the max sentence\n",
    "list_maxlen_sent = sorted(list_maxlen_sent)\n",
    "maxlen_sent = list_maxlen_sent[int(len(list_maxlen_sent)*0.95)]\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_wword = sorted(list_maxlen_word)\n",
    "maxlen_word = list_maxlen_word[int(len(list_maxlen_word)*0.95)]\n",
    "\n",
    "#start to pad\n",
    "train_copus_padded = build_input_data(corpus=train_data_word,max_sents=maxlen_sent,max_words=maxlen_word,vocab2int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  10,    9,   29, ...,    0,    0,    0],\n",
       "       [   2,  119,  194, ...,    8,   76,    3],\n",
       "       [3008,   22, 1183, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copus_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_word[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: b'\\x00\\x00\\x94:\\x00\\x00k\\xba\\x00\\x00\\xa79\\x00\\x00\\xc9:\\x00\\x00\\x91:\\x00\\x00\\xb8\\xba\\x00\\x00\\x00\\xb8\\x00\\x00\\xdc\\xb9\\x00\\x00\\x17\\xba\\x00\\x00\\x8d:\\x00\\x00\\x86\\xba\\x00\\x00\"\\xba\\x00\\x00F\\xba\\x00\\x00\\xb8:\\x00\\x00\\xd7\\xba\\x00\\x00&\\xba\\x00\\x00\\xd6:\\x00\\x00\\x84\\xba\\x00\\x00\\xa6\\xba\\x00\\x00+:\\x00\\x00\\xda\\xb9\\x00\\x00\\x8d\\xba\\x00\\x00\\xc8:\\x00\\x00\\x90\\xb9\\x00\\x00\\x139\\x00\\x00\\xce:\\x00\\x00\\xb2:\\x00\\x00Z\\xba\\x00\\x00\\xb8\\xba\\x00\\x00\\xcf:\\x00\\x00\\x859\\x00\\x00@\\xba\\x00\\x00\\xdd\\xb8\\x00\\x00\\x99\\xba\\x00\\x00\\xcf:\\x00\\x00,:\\x00\\x00-\\xba\\x00\\x00D6\\x00\\x00\\x94:\\x00\\x00\\xe09\\x00\\x00\\xc2\\xb9\\x00\\x00\\x97\\xba\\x00\\x00\\xa6:\\x00\\x00l\\xb6\\x00\\x00\\x8b9\\x00\\x00\\xd3\\xb9\\x00\\x00\\x149\\x00\\x00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-73c779cd0e30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0membedding_matrixes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embedding_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab2int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glove'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-6fbcd84a360c>\u001b[0m in \u001b[0;36mload_embedding_matrix\u001b[1;34m(embed_path, vocab2int, EMBEDDING_DIM, embed_type)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m--> 501\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: b'\\x00\\x00\\x94:\\x00\\x00k\\xba\\x00\\x00\\xa79\\x00\\x00\\xc9:\\x00\\x00\\x91:\\x00\\x00\\xb8\\xba\\x00\\x00\\x00\\xb8\\x00\\x00\\xdc\\xb9\\x00\\x00\\x17\\xba\\x00\\x00\\x8d:\\x00\\x00\\x86\\xba\\x00\\x00\"\\xba\\x00\\x00F\\xba\\x00\\x00\\xb8:\\x00\\x00\\xd7\\xba\\x00\\x00&\\xba\\x00\\x00\\xd6:\\x00\\x00\\x84\\xba\\x00\\x00\\xa6\\xba\\x00\\x00+:\\x00\\x00\\xda\\xb9\\x00\\x00\\x8d\\xba\\x00\\x00\\xc8:\\x00\\x00\\x90\\xb9\\x00\\x00\\x139\\x00\\x00\\xce:\\x00\\x00\\xb2:\\x00\\x00Z\\xba\\x00\\x00\\xb8\\xba\\x00\\x00\\xcf:\\x00\\x00\\x859\\x00\\x00@\\xba\\x00\\x00\\xdd\\xb8\\x00\\x00\\x99\\xba\\x00\\x00\\xcf:\\x00\\x00,:\\x00\\x00-\\xba\\x00\\x00D6\\x00\\x00\\x94:\\x00\\x00\\xe09\\x00\\x00\\xc2\\xb9\\x00\\x00\\x97\\xba\\x00\\x00\\xa6:\\x00\\x00l\\xb6\\x00\\x00\\x8b9\\x00\\x00\\xd3\\xb9\\x00\\x00\\x149\\x00\\x00'"
     ]
    }
   ],
   "source": [
    "# get the embedding matrix\n",
    "#把每个词映射到一个300维度的vector\n",
    "#这个matrix是二维的\n",
    "#用vocab2int中每个词对应的整数来去matrix来找\n",
    "dimension = 300\n",
    "path = 'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
    "embedding_matrixes = load_embedding_matrix(embed_path = path,EMBEDDING_DIM=dimension,vocab2int = vocab_to_int,embed_type='glove')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
