{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "* 1.loading data\n",
    "* 2.tokenizing data to the list\n",
    "* 3.build the vacabulary to integer\n",
    "* 4.padding the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dirname = 'data/aclImdb'\n",
    "# filename = 'aclImdb_v1.tar.gz'\n",
    "# locale.setlocale(locale.LC_ALL, 'C')\n",
    "# all_lines = []\n",
    "\n",
    "# if sys.version > '3':\n",
    "#     control_chars = [chr(0x85)]\n",
    "# else:\n",
    "#     control_chars = [unichr(0x85)]\n",
    "\n",
    "# # Convert text to lower-case and strip punctuation/symbols from words\n",
    "# def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "#     # Replace breaks with spaces\n",
    "#     norm_text = norm_text.replace('<br />', ' ')\n",
    "#     # Pad punctuation with spaces on both sides\n",
    "#     norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "#     return norm_text\n",
    "\n",
    "# if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "#     if not os.path.isdir(dirname):\n",
    "#         if not os.path.isfile(filename):\n",
    "#             # Download IMDB archive\n",
    "#             print(\"Downloading IMDB archive...\")\n",
    "#             url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "#             r = requests.get(url)\n",
    "#             with smart_open(filename, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "#         tar = tarfile.open(filename, mode='r')\n",
    "#         tar.extractall()\n",
    "#         tar.close()\n",
    "#     else:\n",
    "#         print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "#     # Collect & normalize test/train data\n",
    "#     print(\"Cleaning up dataset...\")\n",
    "#     folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "#     for fol in folders:\n",
    "#         temp = u''\n",
    "#         newline = \"\\n\".encode(\"utf-8\")\n",
    "#         output = fol.replace('/', '-') + '.txt'\n",
    "#         # Is there a better pattern to use?\n",
    "#         txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "#         print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "#         with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "#             for i, txt in enumerate(txt_files):\n",
    "#                 with smart_open(txt, \"rb\") as t:\n",
    "#                     one_text = t.read().decode(\"utf-8\")\n",
    "#                     for c in control_chars:\n",
    "#                         one_text = one_text.replace(c, ' ')\n",
    "#                     one_text = normalize_text(one_text)\n",
    "#                     all_lines.append(one_text)\n",
    "#                     n.write(one_text.encode(\"utf-8\"))\n",
    "#                     n.write(newline)\n",
    "\n",
    "#     # Save to disk for instant re-use on any future runs\n",
    "#     with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "#         for idx, line in enumerate(all_lines):\n",
    "#             num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "#             f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "# assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "# print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix(embed_path, vocab2int, EMBEDDING_DIM, embed_type='glove'):\n",
    "    #**********embed_path是glove的URL吗\n",
    "    #**********EMBEDDING_DIM是什么\n",
    "    \"\"\"\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "    if embed_type == 'word2vec': \n",
    "        \"\"\"Loads 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\"\"\"\n",
    "        with open(embed_path, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            vocab_size, layer1_size = map(int, header.split())\n",
    "            binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "            for line in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == ' ':\n",
    "                        word = ''.join(word)\n",
    "                        break\n",
    "                    if ch != '\\n':\n",
    "                        word.append(ch)\n",
    "                word = word.decode('utf-8', 'ignore')\n",
    "                embeddings[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                continue\n",
    "    else:\n",
    "        # load Glove or Dependency-based word embeddings\n",
    "        f = open(embed_path, \"rb\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        f.close()\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('D:/code_stock/SA/data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('D:/code_stock/SA/data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('D:/code_stock/SA/data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('D:/code_stock/SA/data/aclImdb/train-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "tese_data , test_label = shuffle(test_data ,train_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_word1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0444e50d53e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#start to pad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mtrain_copus_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_input_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_word1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_sents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen_sent1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen_word1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab2int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_to_int_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#use pickle to store the data\"train_copus_padded\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_word1' is not defined"
     ]
    }
   ],
   "source": [
    "#**********tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  \n",
    "\n",
    "#**********for training data prepocessing\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        train_data_word[i].append(word_tokenize(train_data_sent[i][j]))  \n",
    "        \n",
    "#building the vacabulary\n",
    "vocab_to_int_train = build_vocab(train_data_word)\n",
    "\n",
    "#get the padding element\n",
    "maxlen_word1 = 0\n",
    "maxlen_sent1 = 0\n",
    "#pad_train_set = []\n",
    "list_maxlen_sent1 = []\n",
    "list_maxlen_word1 = []\n",
    "\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent1.append((len(train_data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word1.append(len(train_data_sent[i][j]))\n",
    "\n",
    "#get the max sentence\n",
    "list_maxlen_sent1 = sorted(list_maxlen_sent1)\n",
    "maxlen_sent1 = list_maxlen_sent1[int(len(list_maxlen_sent1)*0.95)]\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word1)\n",
    "maxlen_word1 = list_maxlen_word1[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "#start to pad\n",
    "train_copus_padded = build_input_data(corpus=train_data_word1,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)\n",
    "\n",
    "#use pickle to store the data\"train_copus_padded\"\n",
    "import pickle\n",
    "train_copus_pad = open('pickle_train_copus_pad.pickle','wb')\n",
    "pickle.dump(train_copus_padded,train_copus_pad)\n",
    "train_copus_pad.close()\n",
    "\n",
    "#**********for testing data prepocessing\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "test_data_word = [[]for i in range(len(test_data_sent))]\n",
    "for i in range(len(test_data_sent)):\n",
    "    for j in range(len(test_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        test_data_word[i].append(word_tokenize(test_data_sent[i][j]))  \n",
    "        \n",
    "#building the vacabulary\n",
    "vocab_to_int_tese = build_vocab(test_data_word)\n",
    "\n",
    "#get the padding element\n",
    "maxlen_word = 0\n",
    "maxlen_sent = 0\n",
    "#pad_test_set = []\n",
    "list_maxlen_sent2 = []\n",
    "list_maxlen_word2 = []\n",
    "\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(test_data_sent)):\n",
    "    list_maxlen_sent2.append((len(test_data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(test_data_sent)):\n",
    "    for j in range(len(test_data_sent[i])):\n",
    "        list_maxlen_word2.append(len(test_data_sent[i][j]))\n",
    "\n",
    "#get the max sentence\n",
    "list_maxlen_sent2 = sorted(list_maxlen_sent2)\n",
    "maxlen_sent2 = list_maxlen_sent2[int(len(list_maxlen_sent2)*0.95)]\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_word2 = sorted(list_maxlen_word2)\n",
    "maxlen_word2 = list_maxlen_word2[int(len(list_maxlen_word2)*0.95)]\n",
    "\n",
    "#start to pad\n",
    "test_copus_padded = build_input_data(corpus=test_data_word,max_sents=maxlen_sent2,max_words=maxlen_word2,vocab2int=vocab_to_int_test)\n",
    "\n",
    "#use pickle to store the data\"train_copus_padded\"\n",
    "import pickle\n",
    "train_copus_pad = open('pickle_train_copus_pad.pickle','wb')\n",
    "pickle.dump(train_copus_padded,train_copus_pad)\n",
    "train_copus_pad.close()\n",
    "test_copus_pad = open('pickle_test_copus_pad.pickle','wb')\n",
    "pickle.dump(test_copus_padded,tese_copus_pad)\n",
    "test_copus_pad.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-22dd00f5a4dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0membedding_matrixes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_embedding_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab2int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-6fbcd84a360c>\u001b[0m in \u001b[0;36mload_embedding_matrix\u001b[1;34m(embed_path, vocab2int, EMBEDDING_DIM, embed_type)\u001b[0m\n\u001b[0;32m     97\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                         \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the embedding matrix\n",
    "#把每个词映射到一个300维度的vector\n",
    "#这个matrix是二维的\n",
    "#用vocab2int中每个词对应的整数来去matrix来找\n",
    "dimension = 300\n",
    "path = 'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
<<<<<<< HEAD
    "embedding_matrixes = load_embedding_matrix(embed_path = path,EMBEDDING_DIM=dimension,vocab2int = vocab_to_int,embed_type='word2vec')\n"
=======
    "embedding_matrixes = load_embedding_matrix_gensim(embed_path = path,EMBEDDING_DIM=dimension,vocab2int = vocab_to_int)\n"
>>>>>>> fc03be840c2d8929f8b184498946969a64d068e3
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
