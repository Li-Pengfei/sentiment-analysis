{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "* 1.loading data\n",
    "* 2.tokenizing data to the list\n",
    "* 3.build the vacabulary to integer\n",
    "* 4.padding the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB archive directory already available without download.\n",
      "Cleaning up dataset...\n",
      " train/pos: 12500 files\n",
      " train/neg: 12500 files\n",
      " test/pos: 12500 files\n",
      " test/neg: 12500 files\n",
      "Success, alldata-id.txt is available for next steps.\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/aclImdb'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('aclImdb/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        if not os.path.isfile(filename):\n",
    "            # Download IMDB archive\n",
    "            print(\"Downloading IMDB archive...\")\n",
    "            url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "            r = requests.get(url)\n",
    "            with smart_open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "        tar = tarfile.open(filename, mode='r')\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline = \"\\n\".encode(\"utf-8\")\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"data/aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix(embed_path, vocab2int, EMBEDDING_DIM, embed_type='glove'):\n",
    "    \"\"\"\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "    if embed_type == 'word2vec': \n",
    "        \"\"\"Loads 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\"\"\"\n",
    "        with open(embed_path, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            vocab_size, layer1_size = map(int, header.split())\n",
    "            binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "            for line in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == ' ':\n",
    "                        word = ''.join(word)\n",
    "                        break\n",
    "                    if ch != '\\n':\n",
    "                        word.append(ch)\n",
    "                word = word.decode('utf-8', 'ignore')\n",
    "                embeddings[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                continue\n",
    "    else:\n",
    "        # load Glove or Dependency-based word embeddings\n",
    "        f = open(embed_path)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        f.close()\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the movie review \"list of string\"\n",
    "test_neg = readfile('D:/code_stock/SA_Code/data/aclImdb/test-neg.txt')\n",
    "test_pos = readfile('D:/code_stock/SA_Code/data/aclImdb/test-pos.txt')\n",
    "train_neg = readfile('D:/code_stock/SA_Code/data/aclImdb/train-neg.txt')\n",
    "train_pos = readfile('D:/code_stock/SA_Code/data/aclImdb/train-pos.txt')\n",
    "\n",
    "# test_neg = readfile('data/aclImdb/test-neg.txt')\n",
    "# test_pos = readfile('data/aclImdb/test-pos.txt')\n",
    "# train_neg = readfile('data/aclImdb/train-neg.txt')\n",
    "# train_pos = readfile('data/aclImdb/train-pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "tese_data , test_label = shuffle(test_data ,train_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        train_data_word[i].append(word_tokenize(train_data_sent[i][j]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************\n",
      "slicing the train_data_word list for obeservation./n\n",
      "[[['what', 'can', 'i', 'add', 'that', 'the', 'previous', 'comments', 'have', \"n't\", 'already', 'said', '.'], ['this', 'is', 'a', 'great', 'film', 'and', 'the', 'light', 'sabre', 'duel', 'star', 'wars', 'tribute', 'has', 'to', 'be', 'seen', 'to', 'be', 'believed', '!'], ['!'], ['there', 'are', 'moments', 'of', 'genius', 'throughout', 'this', 'movie', ',', 'if', 'you', 'can', ',', 'see', 'it', 'now', '!'], ['thanks', 'again', 'to', 'rick', 'baker', 'who', 'gave', 'me', 'this', 'movie', 'many', 'years', 'ago', '!']]]\n",
      "**************************\n"
     ]
    }
   ],
   "source": [
    "print('**************************')\n",
    "print('slicing the train_data_word list for obeservation./n')\n",
    "print(train_data_word[:1])\n",
    "print('**************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************\n",
      "train_data_word[0] =  [['what', 'can', 'i', 'add', 'that', 'the', 'previous', 'comments', 'have', \"n't\", 'already', 'said', '.'], ['this', 'is', 'a', 'great', 'film', 'and', 'the', 'light', 'sabre', 'duel', 'star', 'wars', 'tribute', 'has', 'to', 'be', 'seen', 'to', 'be', 'believed', '!'], ['!'], ['there', 'are', 'moments', 'of', 'genius', 'throughout', 'this', 'movie', ',', 'if', 'you', 'can', ',', 'see', 'it', 'now', '!'], ['thanks', 'again', 'to', 'rick', 'baker', 'who', 'gave', 'me', 'this', 'movie', 'many', 'years', 'ago', '!']]\n",
      "**************************\n",
      "train_data_word[0][0] =  ['what', 'can', 'i', 'add', 'that', 'the', 'previous', 'comments', 'have', \"n't\", 'already', 'said', '.']\n",
      "**************************\n",
      "train_data_word[0][0][0] =  what\n"
     ]
    }
   ],
   "source": [
    "print('**************************')\n",
    "print('train_data_word[0] = ',train_data_word[0])\n",
    "print('**************************')\n",
    "print('train_data_word[0][0] = ',train_data_word[0][0])\n",
    "print('**************************')\n",
    "print('train_data_word[0][0][0] = ',train_data_word[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 335437), ('.', 327192), (',', 276280), ('and', 163554), ('a', 162627), ('of', 145447), ('to', 135228), ('is', 110431), ('it', 96231), ('in', 93326)]\n"
     ]
    }
   ],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int = build_vocab(train_data_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding the sentence\n",
    "maxlen_word = 0\n",
    "maxlen_sent = 0\n",
    "pad_train_set = []\n",
    "list_maxlen_sent = []\n",
    "list_maxlen_word = []\n",
    "\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent.append((len(train_data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word.append(len(train_data_sent[i][j]))\n",
    "\n",
    "#get max_words\n",
    "maxlen_word = max(list_maxlen_word)\n",
    " \n",
    "#get the max_sents\n",
    "maxlen_sent = max(list_maxlen_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
