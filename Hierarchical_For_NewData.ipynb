{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\[\\].\\\",()!?;:/])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  \n",
    "    # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "path_pos = 'data/unprocessed/sorted_data/apparel/positive.review'\n",
    "path_neg = 'data/unprocessed/sorted_data/apparel/negative.review'\n",
    "\n",
    "#read the file\n",
    "file_pos = open(path_pos,'r',encoding='windows-1252')\n",
    "file_pos = file_pos.read()\n",
    "\n",
    "file_neg = open(path_neg,'r',encoding='windows-1252')\n",
    "file_neg = file_neg.read()\n",
    "\n",
    "#extact the file\n",
    "positive = BeautifulSoup(file_pos)\n",
    "positive = positive.find_all('review_text')#get all of the positive reviews\n",
    "for i in range(len(positive)):#convet the elements in postive to the string type\n",
    "    positive[i] = str(positive[i])\n",
    "\n",
    "negative = BeautifulSoup(file_neg)\n",
    "negative = negative.find_all('review_text')#get all the positive reviews\n",
    "for i in range(len(negative)):#convet the elements in negative to the string type\n",
    "    negative[i] = str(negative[i])\n",
    "    \n",
    "#eliminate the <review_text></review_text>tag in the reviews and normalize the text\n",
    "positive_doc = []\n",
    "for review in positive:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    review = review.lower()\n",
    "    positive_doc.append(review)\n",
    "\n",
    "negative_doc = []\n",
    "for review in negative:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    review = review.lower()\n",
    "    negative_doc.append(review)\n",
    "    \n",
    "#merge the data\n",
    "data = negative_doc + positive_doc\n",
    "\n",
    "#get the train label\n",
    "pos_label = [1 for i in range(len(positive_doc))]\n",
    "neg_label = [0 for i in range(len(negative_doc))]\n",
    "train_label = pos_label + neg_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #split the data to train and test dataset\n",
    "# train_copus_padded, test_copus_padded, train_label, test_label = train_test_split(\n",
    "#     copus_padded, train_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# #get the label\n",
    "# train_num = int(len(positive_doc)*0.8)\n",
    "# test_num = int(len(positive_doc)*0.2)\n",
    "# train_pos_label = [1 for i in range (train_num)]\n",
    "# train_neg_label = [0 for i in range(train_num)]\n",
    "# test_pos_label = [1 for i in range(test_num)]\n",
    "# test_neg_label = [0 for i in range(test_num)]\n",
    "\n",
    "# #get the data\n",
    "# train_pos_doc = positive_doc[0:800]\n",
    "# train_neg_doc = negative_doc[0:800]\n",
    "# test_pos_doc = positive_doc[800:]\n",
    "# test_neg_doc = negative_doc[800:]\n",
    "\n",
    "# #merge the data\n",
    "# train_data = train_pos_doc + train_neg_doc\n",
    "# train_label = train_pos_label + train_neg_label\n",
    "# test_data = test_pos_doc + test_neg_doc\n",
    "# test_label = test_pos_label + test_neg_label\n",
    "\n",
    "# #shuffle the data\n",
    "# from sklearn.utils import shuffle \n",
    "# train_data , train_label = shuffle(train_data , train_label , random_state = 0) \n",
    "# test_data, test_label = shuffle(test_data, test_label, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data for hierarchical model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the doc list to the list of sentences\n",
    "data_sent = [sent_tokenize(data[i]) for i in range(len(data))]\n",
    "\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "data_word = [[]for i in range(len(data_sent))]\n",
    "for i in range(len(data_sent)):\n",
    "    for j in range(len(data_sent[i])):\n",
    "        word_tokens = word_tokenize(data_sent[i][j])\n",
    "        if word_tokens != []:\n",
    "            data_word[i].append(word_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int = build_vocab(data_word)\n",
    "\n",
    "#get the list which is the maxim quantity of sentence'\n",
    "#get the padding element\n",
    "maxlen_word = 0\n",
    "maxlen_sent = 0\n",
    "\n",
    "list_maxlen_sent = []\n",
    "list_maxlen_word = []\n",
    "for i in range(len(data_sent)):\n",
    "    list_maxlen_sent.append((len(data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(data_sent)):\n",
    "    for j in range(len(data_sent[i])):\n",
    "        list_maxlen_word.append(len(data_sent[i][j]))\n",
    "\n",
    "#get the max sentence\n",
    "list_maxlen_sent = sorted(list_maxlen_sent)\n",
    "maxlen_sent = list_maxlen_sent[int(len(list_maxlen_sent)*0.95)]\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word)\n",
    "maxlen_word = list_maxlen_word[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "\n",
    "#start to pad\n",
    "copus_padded = build_input_data(corpus=data_word,max_sents=maxlen_sent,max_words=maxlen_word,vocab2int=vocab_to_int)\n",
    "# test_copus_padded = build_input_data(corpus=test_data_word,max_sents=maxlen_sent,max_words=maxlen_word,vocab2int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('pickle_New_Data/vocab_to_int.pickle','wb')     \n",
    "pickle.dump(vocab_to_int,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data to train and test dataset\n",
    "train_copus_padded, test_copus_padded, train_label, test_label = train_test_split(\n",
    "copus_padded, train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # get the embedding matrix\n",
    "# # the number of the vocabulary is 100327\n",
    "# # 把每个词映射到一个300维度的vector\n",
    "# # 这个matrix是二维的\n",
    "# # 用vocab2int中每个词对应的整数来去matrix来找对应的vector\n",
    "# dimension = 300\n",
    "# path = 'E:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
    "# embedding_matrix = load_embedding_matrix_gensim(embed_path = path,vocab2int=vocab_to_int,EMBEDDING_DIM=dimension)\n",
    "\n",
    "# #use pickle to store the data\n",
    "# file = open('pickle_New_Data/embedding_matrix.pickle','wb')     \n",
    "# pickle.dump(embedding_matrix,file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('pickle_New_Data/CNN_Weights.pickle','rb')\n",
    "CNN_Weights = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_New_Data/embedding_matrix.pickle','rb')\n",
    "embedding_matrix = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1600, 13, 23)\n",
      "test data shape: (400, 13, 23)\n",
      "embedding_matrix shape: (8023, 300)\n",
      "vocabulary size: 8023\n",
      "max sent length: 13 \n",
      "max word length: 23\n"
     ]
    }
   ],
   "source": [
    "#the shape of the data\n",
    "print('train data shape:',train_copus_padded.shape)\n",
    "print('test data shape:',test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "maxlen_sent = train_copus_padded.shape[1]\n",
    "maxlen_word = train_copus_padded.shape[2]\n",
    "print('max sent length:', maxlen_sent, '\\nmax word length:', maxlen_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building hierarchical models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 128\n",
    "dropout_rate = 0.3\n",
    "atten_dim = 50\n",
    "\n",
    "batch_size = 50\n",
    "epoch_num = 15\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "    test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # hyper-parameters in IMDB\n",
    "# gru_dim = 128\n",
    "# dropout_rate = 0.2\n",
    "# atten_dim = 100\n",
    "\n",
    "# batch_size = 100\n",
    "# epoch_num = 15\n",
    "\n",
    "# categorical_label = True\n",
    "\n",
    "# if categorical_label:\n",
    "#     train_label_cat = np_utils.to_categorical(train_label)\n",
    "#     test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+biGRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 23, 300)           2406900   \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 23, 100)           90100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 2,497,000\n",
      "Trainable params: 90,100\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 13, 23)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 13, 100)           2497000   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 13, 256)           175872    \n",
      "_________________________________________________________________\n",
      "att_layer_11 (AttLayer)      (None, 256)               12900     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,686,286\n",
      "Trainable params: 279,386\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length= maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "\n",
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(maxlen_word,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn = cnn_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn = max_pooling_layer(sent_cnn)  # output shape: (None, nb_filter)\n",
    "\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()\n",
    "\n",
    "# build document encoder model\n",
    "review_input = Input(shape=(maxlen_sent, maxlen_word), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "rnn_out = rnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(att_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(att_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.6801 - acc: 0.5700\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.7625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7008    0.9204    0.7957       201\n",
      "           1     0.8824    0.6030    0.7164       199\n",
      "\n",
      "   micro avg     0.7625    0.7625    0.7625       400\n",
      "   macro avg     0.7916    0.7617    0.7561       400\n",
      "weighted avg     0.7911    0.7625    0.7563       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.4753 - acc: 0.8019\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8300\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7930    0.8955    0.8411       201\n",
      "           1     0.8786    0.7638    0.8172       199\n",
      "\n",
      "   micro avg     0.8300    0.8300    0.8300       400\n",
      "   macro avg     0.8358    0.8297    0.8292       400\n",
      "weighted avg     0.8356    0.8300    0.8292       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.2906 - acc: 0.8819\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8371    0.9204    0.8768       201\n",
      "           1     0.9106    0.8191    0.8624       199\n",
      "\n",
      "   micro avg     0.8700    0.8700    0.8700       400\n",
      "   macro avg     0.8739    0.8697    0.8696       400\n",
      "weighted avg     0.8737    0.8700    0.8696       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 5s 3ms/step - loss: 0.1889 - acc: 0.9244\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8789    0.8308    0.8542       201\n",
      "           1     0.8381    0.8844    0.8606       199\n",
      "\n",
      "   micro avg     0.8575    0.8575    0.8575       400\n",
      "   macro avg     0.8585    0.8576    0.8574       400\n",
      "weighted avg     0.8586    0.8575    0.8574       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 3ms/step - loss: 0.1283 - acc: 0.9550\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8632    0.9104    0.8862       201\n",
      "           1     0.9043    0.8543    0.8786       199\n",
      "\n",
      "   micro avg     0.8825    0.8825    0.8825       400\n",
      "   macro avg     0.8837    0.8824    0.8824       400\n",
      "weighted avg     0.8836    0.8825    0.8824       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0687 - acc: 0.9775\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8409    0.9204    0.8789       201\n",
      "           1     0.9111    0.8241    0.8654       199\n",
      "\n",
      "   micro avg     0.8725    0.8725    0.8725       400\n",
      "   macro avg     0.8760    0.8723    0.8721       400\n",
      "weighted avg     0.8758    0.8725    0.8722       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0397 - acc: 0.9881\n",
      "Evaluating...\n",
      "400/400 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.8750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8386    0.9303    0.8821       201\n",
      "           1     0.9209    0.8191    0.8670       199\n",
      "\n",
      "   micro avg     0.8750    0.8750    0.8750       400\n",
      "   macro avg     0.8797    0.8747    0.8745       400\n",
      "weighted avg     0.8795    0.8750    0.8746       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0755 - acc: 0.9769\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8438    0.9403    0.8894       201\n",
      "           1     0.9318    0.8241    0.8747       199\n",
      "\n",
      "   micro avg     0.8825    0.8825    0.8825       400\n",
      "   macro avg     0.8878    0.8822    0.8820       400\n",
      "weighted avg     0.8876    0.8825    0.8821       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0292 - acc: 0.9950\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8500    0.9303    0.8884       201\n",
      "           1     0.9222    0.8342    0.8760       199\n",
      "\n",
      "   micro avg     0.8825    0.8825    0.8825       400\n",
      "   macro avg     0.8861    0.8823    0.8822       400\n",
      "weighted avg     0.8859    0.8825    0.8822       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.0170 - acc: 0.9938\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8705    0.8358    0.8528       201\n",
      "           1     0.8406    0.8744    0.8571       199\n",
      "\n",
      "   micro avg     0.8550    0.8550    0.8550       400\n",
      "   macro avg     0.8555    0.8551    0.8550       400\n",
      "weighted avg     0.8556    0.8550    0.8550       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0138 - acc: 0.9969\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.7975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9110    0.6617    0.7666       201\n",
      "           1     0.7323    0.9347    0.8212       199\n",
      "\n",
      "   micro avg     0.7975    0.7975    0.7975       400\n",
      "   macro avg     0.8216    0.7982    0.7939       400\n",
      "weighted avg     0.8221    0.7975    0.7937       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0452 - acc: 0.9837\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8756    0.8408    0.8579       201\n",
      "           1     0.8454    0.8794    0.8621       199\n",
      "\n",
      "   micro avg     0.8600    0.8600    0.8600       400\n",
      "   macro avg     0.8605    0.8601    0.8600       400\n",
      "weighted avg     0.8606    0.8600    0.8600       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0382 - acc: 0.9838\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8371    0.9204    0.8768       201\n",
      "           1     0.9106    0.8191    0.8624       199\n",
      "\n",
      "   micro avg     0.8700    0.8700    0.8700       400\n",
      "   macro avg     0.8739    0.8697    0.8696       400\n",
      "weighted avg     0.8737    0.8700    0.8696       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0167 - acc: 0.9963\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8139    0.9353    0.8704       201\n",
      "           1     0.9231    0.7839    0.8478       199\n",
      "\n",
      "   micro avg     0.8600    0.8600    0.8600       400\n",
      "   macro avg     0.8685    0.8596    0.8591       400\n",
      "weighted avg     0.8682    0.8600    0.8592       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 6s 4ms/step - loss: 0.0139 - acc: 0.9969\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.8325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8851    0.7662    0.8213       201\n",
      "           1     0.7920    0.8995    0.8424       199\n",
      "\n",
      "   micro avg     0.8325    0.8325    0.8325       400\n",
      "   macro avg     0.8385    0.8328    0.8318       400\n",
      "weighted avg     0.8388    0.8325    0.8318       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training for epoch 5/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 6s 3ms/step - loss: 0.1283 - acc: 0.9550\n",
    "Evaluating...\n",
    "400/400 [==============================] - 1s 2ms/step\n",
    "Accuracy: 0.8825\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8632    0.9104    0.8862       201\n",
    "           1     0.9043    0.8543    0.8786       199\n",
    "\n",
    "   micro avg     0.8825    0.8825    0.8825       400\n",
    "   macro avg     0.8837    0.8824    0.8824       400\n",
    "weighted avg     0.8836    0.8825    0.8824       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-CNN+biGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=50, kernel_size=3, strides=1, padding=\"same\")`\n",
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", weights=[array([[[..., trainable=False, filters=533, kernel_size=1, strides=1, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer1 = Convolution1D(nb_filter=50,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_Weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                           weights = CNN_Weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 23, 300)      2406900     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 23, 50)       45050       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 23, 533)      160433      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM multiple             0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 583)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_5[1][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,612,383\n",
      "Trainable params: 45,050\n",
      "Non-trainable params: 2,567,333\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(maxlen_word,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 13, 23)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 13, 583)           2612383   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 13, 256)           546816    \n",
      "_________________________________________________________________\n",
      "att_layer_5 (AttLayer)       (None, 256)               12900     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 3,172,613\n",
      "Trainable params: 605,280\n",
      "Non-trainable params: 2,567,333\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build document encoder model\n",
    "review_input = Input(shape=(maxlen_sent, maxlen_word), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "rnn_out = rnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(att_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(att_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.7156 - acc: 0.5119\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 4ms/step\n",
      "Accuracy: 0.6200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7634    0.3532    0.4830       201\n",
      "           1     0.5765    0.8894    0.6996       199\n",
      "\n",
      "   micro avg     0.6200    0.6200    0.6200       400\n",
      "   macro avg     0.6700    0.6213    0.5913       400\n",
      "weighted avg     0.6705    0.6200    0.5908       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.6844 - acc: 0.5637\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.6525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6115    0.8458    0.7098       201\n",
      "           1     0.7459    0.4573    0.5670       199\n",
      "\n",
      "   micro avg     0.6525    0.6525    0.6525       400\n",
      "   macro avg     0.6787    0.6515    0.6384       400\n",
      "weighted avg     0.6784    0.6525    0.6388       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.6558 - acc: 0.6131\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.6625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9583    0.3433    0.5055       201\n",
      "           1     0.5976    0.9849    0.7438       199\n",
      "\n",
      "   micro avg     0.6625    0.6625    0.6625       400\n",
      "   macro avg     0.7779    0.6641    0.6247       400\n",
      "weighted avg     0.7788    0.6625    0.6241       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 4ms/step - loss: 0.4722 - acc: 0.7869\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8564    0.8607    0.8586       201\n",
      "           1     0.8586    0.8543    0.8564       199\n",
      "\n",
      "   micro avg     0.8575    0.8575    0.8575       400\n",
      "   macro avg     0.8575    0.8575    0.8575       400\n",
      "weighted avg     0.8575    0.8575    0.8575       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 7s 5ms/step - loss: 0.3508 - acc: 0.8487\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7659    0.9602    0.8521       201\n",
      "           1     0.9459    0.7035    0.8069       199\n",
      "\n",
      "   micro avg     0.8325    0.8325    0.8325       400\n",
      "   macro avg     0.8559    0.8319    0.8295       400\n",
      "weighted avg     0.8555    0.8325    0.8296       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.2941 - acc: 0.8787\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8846    0.8010    0.8407       201\n",
      "           1     0.8165    0.8945    0.8537       199\n",
      "\n",
      "   micro avg     0.8475    0.8475    0.8475       400\n",
      "   macro avg     0.8506    0.8477    0.8472       400\n",
      "weighted avg     0.8507    0.8475    0.8472       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.2361 - acc: 0.9000\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.8725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8151    0.9652    0.8838       201\n",
      "           1     0.9568    0.7789    0.8587       199\n",
      "\n",
      "   micro avg     0.8725    0.8725    0.8725       400\n",
      "   macro avg     0.8860    0.8720    0.8713       400\n",
      "weighted avg     0.8856    0.8725    0.8713       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.1957 - acc: 0.9169\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8730    0.8209    0.8462       201\n",
      "           1     0.8294    0.8794    0.8537       199\n",
      "\n",
      "   micro avg     0.8500    0.8500    0.8500       400\n",
      "   macro avg     0.8512    0.8501    0.8499       400\n",
      "weighted avg     0.8513    0.8500    0.8499       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.1521 - acc: 0.9419\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8756    0.8756    0.8756       201\n",
      "           1     0.8744    0.8744    0.8744       199\n",
      "\n",
      "   micro avg     0.8750    0.8750    0.8750       400\n",
      "   macro avg     0.8750    0.8750    0.8750       400\n",
      "weighted avg     0.8750    0.8750    0.8750       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 9s 5ms/step - loss: 0.1229 - acc: 0.9525\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8929    0.7463    0.8130       201\n",
      "           1     0.7802    0.9095    0.8399       199\n",
      "\n",
      "   micro avg     0.8275    0.8275    0.8275       400\n",
      "   macro avg     0.8365    0.8279    0.8265       400\n",
      "weighted avg     0.8368    0.8275    0.8264       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0878 - acc: 0.9650\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8213    0.9602    0.8853       201\n",
      "           1     0.9515    0.7889    0.8626       199\n",
      "\n",
      "   micro avg     0.8750    0.8750    0.8750       400\n",
      "   macro avg     0.8864    0.8746    0.8740       400\n",
      "weighted avg     0.8861    0.8750    0.8740       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0607 - acc: 0.9813\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8731    0.8557    0.8643       201\n",
      "           1     0.8571    0.8744    0.8657       199\n",
      "\n",
      "   micro avg     0.8650    0.8650    0.8650       400\n",
      "   macro avg     0.8651    0.8650    0.8650       400\n",
      "weighted avg     0.8652    0.8650    0.8650       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0487 - acc: 0.9794\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8700    0.8657    0.8678       201\n",
      "           1     0.8650    0.8693    0.8672       199\n",
      "\n",
      "   micro avg     0.8675    0.8675    0.8675       400\n",
      "   macro avg     0.8675    0.8675    0.8675       400\n",
      "weighted avg     0.8675    0.8675    0.8675       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0336 - acc: 0.9900\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8393    0.9353    0.8847       201\n",
      "           1     0.9261    0.8191    0.8693       199\n",
      "\n",
      "   micro avg     0.8775    0.8775    0.8775       400\n",
      "   macro avg     0.8827    0.8772    0.8770       400\n",
      "weighted avg     0.8825    0.8775    0.8771       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0612 - acc: 0.9750\n",
      "Evaluating...\n",
      "400/400 [==============================] - 1s 3ms/step\n",
      "Accuracy: 0.8700\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8433    0.9104    0.8756       201\n",
      "           1     0.9016    0.8291    0.8639       199\n",
      "\n",
      "   micro avg     0.8700    0.8700    0.8700       400\n",
      "   macro avg     0.8725    0.8698    0.8697       400\n",
      "weighted avg     0.8723    0.8700    0.8698       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1: \n",
    "        \n",
    "        \n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training for epoch 11/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 8s 5ms/step - loss: 0.0953 - acc: 0.9631\n",
    "Evaluating...\n",
    "400/400 [==============================] - 1s 3ms/step\n",
    "Accuracy: 0.8925\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8527    0.9502    0.8988       201\n",
    "           1     0.9432    0.8342    0.8853       199\n",
    "\n",
    "   micro avg     0.8925    0.8925    0.8925       400\n",
    "   macro avg     0.8979    0.8922    0.8921       400\n",
    "weighted avg     0.8977    0.8925    0.8921       400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biGRU+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 23, 300)           2406900   \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 256)               329472    \n",
      "=================================================================\n",
      "Total params: 2,736,372\n",
      "Trainable params: 329,472\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 13, 23)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 13, 256)           2736372   \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 13, 100)           76900     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,813,474\n",
      "Trainable params: 406,574\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length= maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "\n",
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(maxlen_word,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_biGRU = rnn_layer(sent_embedding)\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_biGRU)\n",
    "sentEncoder.summary()\n",
    "\n",
    "# build document encoder model\n",
    "review_input = Input(shape=(maxlen_sent, maxlen_word), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "cnn_out = cnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "cnn_out = max_pooling_layer(cnn_out)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(cnn_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(cnn_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 28s 18ms/step - loss: 0.6243 - acc: 0.6506\n",
      "Evaluating...\n",
      "400/400 [==============================] - 4s 9ms/step\n",
      "Accuracy: 0.7225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7885    0.6119    0.6891       201\n",
      "           1     0.6803    0.8342    0.7494       199\n",
      "\n",
      "   micro avg     0.7225    0.7225    0.7225       400\n",
      "   macro avg     0.7344    0.7231    0.7193       400\n",
      "weighted avg     0.7347    0.7225    0.7191       400\n",
      "\n",
      "Training for epoch 2/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 22s 14ms/step - loss: 0.4604 - acc: 0.7794\n",
      "Evaluating...\n",
      "400/400 [==============================] - 2s 6ms/step\n",
      "Accuracy: 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8177    0.7363    0.7749       201\n",
      "           1     0.7580    0.8342    0.7943       199\n",
      "\n",
      "   micro avg     0.7850    0.7850    0.7850       400\n",
      "   macro avg     0.7878    0.7852    0.7846       400\n",
      "weighted avg     0.7880    0.7850    0.7845       400\n",
      "\n",
      "Training for epoch 3/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 23s 14ms/step - loss: 0.3913 - acc: 0.8269\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8596    0.7612    0.8074       201\n",
      "           1     0.7838    0.8744    0.8266       199\n",
      "\n",
      "   micro avg     0.8175    0.8175    0.8175       400\n",
      "   macro avg     0.8217    0.8178    0.8170       400\n",
      "weighted avg     0.8219    0.8175    0.8169       400\n",
      "\n",
      "Training for epoch 4/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 0.3632 - acc: 0.8431\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8063    0.8905    0.8463       201\n",
      "           1     0.8764    0.7839    0.8276       199\n",
      "\n",
      "   micro avg     0.8375    0.8375    0.8375       400\n",
      "   macro avg     0.8414    0.8372    0.8370       400\n",
      "weighted avg     0.8412    0.8375    0.8370       400\n",
      "\n",
      "Training for epoch 5/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 0.3167 - acc: 0.8662\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8696    0.7960    0.8312       201\n",
      "           1     0.8102    0.8794    0.8434       199\n",
      "\n",
      "   micro avg     0.8375    0.8375    0.8375       400\n",
      "   macro avg     0.8399    0.8377    0.8373       400\n",
      "weighted avg     0.8400    0.8375    0.8372       400\n",
      "\n",
      "Training for epoch 6/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 0.3238 - acc: 0.8637\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 6ms/step\n",
      "Accuracy: 0.8275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8056    0.8657    0.8345       201\n",
      "           1     0.8533    0.7889    0.8198       199\n",
      "\n",
      "   micro avg     0.8275    0.8275    0.8275       400\n",
      "   macro avg     0.8294    0.8273    0.8272       400\n",
      "weighted avg     0.8293    0.8275    0.8272       400\n",
      "\n",
      "Training for epoch 7/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 0.2739 - acc: 0.8837\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8349    0.8806    0.8571       201\n",
      "           1     0.8723    0.8241    0.8475       199\n",
      "\n",
      "   micro avg     0.8525    0.8525    0.8525       400\n",
      "   macro avg     0.8536    0.8524    0.8523       400\n",
      "weighted avg     0.8535    0.8525    0.8524       400\n",
      "\n",
      "Training for epoch 8/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 25s 16ms/step - loss: 0.2383 - acc: 0.9056\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8492    0.8408    0.8450       201\n",
      "           1     0.8408    0.8492    0.8450       199\n",
      "\n",
      "   micro avg     0.8450    0.8450    0.8450       400\n",
      "   macro avg     0.8450    0.8450    0.8450       400\n",
      "weighted avg     0.8450    0.8450    0.8450       400\n",
      "\n",
      "Training for epoch 9/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 23s 15ms/step - loss: 0.2234 - acc: 0.9094\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8866    0.8557    0.8709       201\n",
      "           1     0.8592    0.8894    0.8741       199\n",
      "\n",
      "   micro avg     0.8725    0.8725    0.8725       400\n",
      "   macro avg     0.8729    0.8726    0.8725       400\n",
      "weighted avg     0.8730    0.8725    0.8725       400\n",
      "\n",
      "Training for epoch 10/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 0.1980 - acc: 0.9225\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8488    0.8657    0.8571       201\n",
      "           1     0.8615    0.8442    0.8528       199\n",
      "\n",
      "   micro avg     0.8550    0.8550    0.8550       400\n",
      "   macro avg     0.8552    0.8549    0.8550       400\n",
      "weighted avg     0.8551    0.8550    0.8550       400\n",
      "\n",
      "Training for epoch 11/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 23s 14ms/step - loss: 0.1863 - acc: 0.9294\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8350\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7922    0.9104    0.8472       201\n",
      "           1     0.8935    0.7588    0.8207       199\n",
      "\n",
      "   micro avg     0.8350    0.8350    0.8350       400\n",
      "   macro avg     0.8428    0.8346    0.8339       400\n",
      "weighted avg     0.8426    0.8350    0.8340       400\n",
      "\n",
      "Training for epoch 12/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 23s 15ms/step - loss: 0.1875 - acc: 0.9200\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8080    0.9005    0.8518       201\n",
      "           1     0.8864    0.7839    0.8320       199\n",
      "\n",
      "   micro avg     0.8425    0.8425    0.8425       400\n",
      "   macro avg     0.8472    0.8422    0.8419       400\n",
      "weighted avg     0.8470    0.8425    0.8419       400\n",
      "\n",
      "Training for epoch 13/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 24s 15ms/step - loss: 0.1435 - acc: 0.9519\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8249    0.8905    0.8565       201\n",
      "           1     0.8798    0.8090    0.8429       199\n",
      "\n",
      "   micro avg     0.8500    0.8500    0.8500       400\n",
      "   macro avg     0.8523    0.8498    0.8497       400\n",
      "weighted avg     0.8522    0.8500    0.8497       400\n",
      "\n",
      "Training for epoch 14/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 28s 17ms/step - loss: 0.1314 - acc: 0.9494\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 7ms/step\n",
      "Accuracy: 0.8450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8174    0.8905    0.8524       201\n",
      "           1     0.8785    0.7990    0.8368       199\n",
      "\n",
      "   micro avg     0.8450    0.8450    0.8450       400\n",
      "   macro avg     0.8479    0.8448    0.8446       400\n",
      "weighted avg     0.8477    0.8450    0.8447       400\n",
      "\n",
      "Training for epoch 15/15\n",
      "Epoch 1/1\n",
      "1600/1600 [==============================] - 26s 16ms/step - loss: 0.1051 - acc: 0.9606\n",
      "Evaluating...\n",
      "400/400 [==============================] - 3s 8ms/step\n",
      "Accuracy: 0.8550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8389    0.8806    0.8592       201\n",
      "           1     0.8730    0.8291    0.8505       199\n",
      "\n",
      "   micro avg     0.8550    0.8550    0.8550       400\n",
      "   macro avg     0.8559    0.8549    0.8549       400\n",
      "weighted avg     0.8559    0.8550    0.8549       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training for epoch 9/15\n",
    "Epoch 1/1\n",
    "1600/1600 [==============================] - 23s 15ms/step - loss: 0.2234 - acc: 0.9094\n",
    "Evaluating...\n",
    "400/400 [==============================] - 3s 7ms/step\n",
    "Accuracy: 0.8725\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8866    0.8557    0.8709       201\n",
    "           1     0.8592    0.8894    0.8741       199\n",
    "\n",
    "   micro avg     0.8725    0.8725    0.8725       400\n",
    "   macro avg     0.8729    0.8726    0.8725       400\n",
    "weighted avg     0.8730    0.8725    0.8725       400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
