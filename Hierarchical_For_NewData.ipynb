{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\[\\].\\\",()!?;:/])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  \n",
    "    # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='UTF-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "path_pos = 'data/unprocessed/sorted_data/apparel/positive.review'\n",
    "path_neg = 'data/unprocessed/sorted_data/apparel/negative.review'\n",
    "\n",
    "#read the file\n",
    "file_pos = open(path_pos,'r',encoding='windows-1252')\n",
    "file_pos = file_pos.read()\n",
    "\n",
    "file_neg = open(path_neg,'r',encoding='windows-1252')\n",
    "file_neg = file_neg.read()\n",
    "\n",
    "#extact the file\n",
    "positive = BeautifulSoup(file_pos)\n",
    "positive = positive.find_all('review_text')#get all of the positive reviews\n",
    "for i in range(len(positive)):#convet the elements in postive to the string type\n",
    "    positive[i] = str(positive[i])\n",
    "\n",
    "negative = BeautifulSoup(file_neg)\n",
    "negative = negative.find_all('review_text')#get all the positive reviews\n",
    "for i in range(len(negative)):#convet the elements in negative to the string type\n",
    "    negative[i] = str(negative[i])\n",
    "    \n",
    "#eliminate the <review_text></review_text>tag in the reviews and normalize the text\n",
    "positive_doc = []\n",
    "for review in positive:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    positive_doc.append(review)\n",
    "\n",
    "negative_doc = []\n",
    "for review in negative:\n",
    "    review = normalize_text(review[14:-15])\n",
    "    negative_doc.append(review)\n",
    "\n",
    "#create the lable\n",
    "train_pos_label =[1 for i in range(len(positive_doc))]\n",
    "train_neg_label =[0 for i in range(len(negative_doc))]\n",
    "    \n",
    "#merge the data\n",
    "train_data = positive_doc + negative_doc\n",
    "train_label = train_pos_label + train_neg_label\n",
    "\n",
    "#shuffle the data\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize the doc list to the list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        word_tokens = word_tokenize(train_data_sent[i][j])\n",
    "        if word_tokens != []:\n",
    "            train_data_word[i].append(word_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int = build_vocab(train_data_word)\n",
    "\n",
    "#get the list which is the maxim quantity of sentence'\n",
    "#get the padding element\n",
    "maxlen_word = 0\n",
    "maxlen_sent = 0\n",
    "\n",
    "list_maxlen_sent = []\n",
    "list_maxlen_word = []\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent.append((len(train_data_sent[i])))\n",
    "\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word.append(len(train_data_sent[i][j]))\n",
    "\n",
    "#get the max sentence\n",
    "list_maxlen_sent = sorted(list_maxlen_sent)\n",
    "maxlen_sent = list_maxlen_sent[int(len(list_maxlen_sent)*0.95)]\n",
    "\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word)\n",
    "maxlen_word = list_maxlen_word[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "\n",
    "#start to pad\n",
    "copus_padded = build_input_data(corpus=train_data_word,max_sents=maxlen_sent,max_words=maxlen_word,vocab2int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 13, 112)\n",
      " .  .  .  .  .  .  .  .  .  .  .  .  . they sent me the wrong size .  .  . and didn't leave clear returning guidelines .  You are better off getting something cheaper ,  because this sure as hell wasnt worth the buck \n",
      "\n",
      "[['they', 'sent', 'me', 'the', 'wrong', 'size'], ['and', 'did', \"n't\", 'leave', 'clear', 'returning', 'guidelines'], ['you', 'are', 'better', 'off', 'getting', 'something', 'cheaper', 'because', 'this', 'sure', 'as', 'hell', 'wasnt', 'worth', 'the', 'buck']] \n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [ 12 255  48 ...   0   0   0]\n",
      " [  4  70  26 ...   0   0   0]\n",
      " [ 19  18 135 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#display the data\n",
    "print(copus_padded.shape)\n",
    "print(train_data[8], '\\n')\n",
    "print(train_data_word[8], '\\n')\n",
    "print(copus_padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding matrix\n",
    "# the number of the vocabulary is 100327\n",
    "# 把每个词映射到一个300维度的vector\n",
    "# 这个matrix是二维的\n",
    "# 用vocab2int中每个词对应的整数来去matrix来找对应的vector\n",
    "dimension = 300\n",
    "path = 'E:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
    "embedding_matrix = load_embedding_matrix_gensim(embed_path = path,vocab2int=vocab_to_int,EMBEDDING_DIM=dimension)\n",
    "\n",
    "#use pickle to store the data\n",
    "file = open('pickle_New_Data/embedding_matrix','wb')     \n",
    "pickle.dump(embedding_matrix,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos lexicon no.: 161\n",
      "neg lexicon no.: 372\n"
     ]
    }
   ],
   "source": [
    "##### extract sentiment lexicons from SentiWordNet\n",
    "file_path = 'resource/SentiWordNet_3.0.0_20130122.txt'\n",
    "content = []\n",
    "with open(file_path,'r',encoding='UTF-8') as f:\n",
    "    while(1):\n",
    "        lines = f.readline()\n",
    "        if(lines[0] is ('a' or 'n' or 'v' or 'r')):\n",
    "            content.append(lines.split('\\t'))\n",
    "        elif(lines[0] is '#'):\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "             \n",
    "#get the sentiment words(score>0.8)\n",
    "neg_words1 = []\n",
    "pos_words1 = []\n",
    "for lines in content:\n",
    "    if(float(lines[2])>=0.8):\n",
    "        pos_words1.append(lines[4])\n",
    "    elif(float(lines[3])>=0.8):\n",
    "        neg_words1.append(lines[4])\n",
    "\n",
    "#get the positive sentiment words\n",
    "#seperate the multiple words\n",
    "pos_words2 = []\n",
    "for i in range(len(pos_words1)):\n",
    "    pos_words2.append(pos_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "pos_words3 = []\n",
    "for i in pos_words2:\n",
    "    for j in i:\n",
    "        pos_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "pos_words4 = []\n",
    "for elem in pos_words3:\n",
    "    pos_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "pos_lexicon = []\n",
    "for elem in pos_words4:\n",
    "    pos_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "    \n",
    "#get the negative sentiment words\n",
    "#seperate the multiple words\n",
    "neg_words2 = []\n",
    "for i in range(len(neg_words1)):\n",
    "    neg_words2.append(neg_words1[i].split(' '))\n",
    "\n",
    "#get the one dimension elements\n",
    "neg_words3 = []\n",
    "for i in neg_words2:\n",
    "    for j in i:\n",
    "        neg_words3.append(j)\n",
    "\n",
    "#delete the '#4'\n",
    "neg_words4 = []\n",
    "for elem in neg_words3:\n",
    "    neg_words4.append(re.sub('#\\d','',elem))\n",
    "    \n",
    "#delete the 'number'\n",
    "neg_lexicon = []\n",
    "for elem in neg_words4:\n",
    "    neg_lexicon.append(re.sub('\\d','',elem))\n",
    "\n",
    "\n",
    "print('pos lexicon no.:', len(pos_lexicon))\n",
    "print('neg lexicon no.:', len(neg_lexicon))\n",
    "\n",
    "#merge the positive and negative words\n",
    "senti_lexicon = pos_lexicon + neg_lexicon\n",
    "\n",
    "\n",
    "#create sentiment word filters\n",
    "#map the sentiment words to integer based on vocab2int\n",
    "senti2int = [getWordIdx(word, vocab_to_int) for word in senti_lexicon]\n",
    "\n",
    "#get the filter weights based on the sentiment words&vocab2int&embedding_matrix\n",
    "def Find_Filter_Weight(senti2int):\n",
    "    \"\"\"sentiwords is the list\"\"\"\n",
    "    word_filter_weights = []\n",
    "    bias_weights = []\n",
    "    filter_len = 1\n",
    "    for i in senti2int:\n",
    "        vector = embedding_matrix[i]  # shape: 300\n",
    "        vector = np.expand_dims(vector, axis=0) #shape: 1x 300\n",
    "        vector = np.expand_dims(vector, axis=2) #shape: 1x 300 x 1\n",
    "        if len(word_filter_weights) == 0:\n",
    "            word_filter_weights = vector\n",
    "        else:\n",
    "            word_filter_weights = np.concatenate((word_filter_weights, vector), axis=2)\n",
    "    #shape is (1, 300, 533)\n",
    "    \n",
    "    bias_weights = np.zeros(len(senti2int))\n",
    "    cnn_wordfilter_weights = [word_filter_weights, bias_weights]\n",
    "    \n",
    "    return cnn_wordfilter_weights    \n",
    "\n",
    "CNN_weights = Find_Filter_Weight(senti2int)\n",
    "\n",
    "#store CNN_weights in pickle\n",
    "file = open('pickle_New_Data/CNN_Weights.pickle','wb')     \n",
    "pickle.dump(CNN_weights,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 533)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#split the data to train and test dataset\n",
    "train_copus_padded, test_copus_padded = train_test_split(copus_padded, train_size=0.9)\n",
    "train_label, test_label = train_test_split(train_label,train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1800, 13, 112)\n",
      "test data shape: (200, 13, 112)\n",
      "embedding_matrix shape: (8023, 300)\n",
      "vocabulary size: 8023\n",
      "max sent length: 13 \n",
      "max word length: 112\n"
     ]
    }
   ],
   "source": [
    "#the shape of the data\n",
    "print('train data shape:',train_copus_padded.shape)\n",
    "print('test data shape:',test_copus_padded.shape)\n",
    "print('embedding_matrix shape:', embedding_matrix.shape)\n",
    "#the size of vocabulary\n",
    "vocab_size = len(vocab_to_int)\n",
    "print('vocabulary size:', vocab_size)\n",
    "# the maximal length of every sentence\n",
    "maxlen_sent = train_copus_padded.shape[1]\n",
    "maxlen_word = train_copus_padded.shape[2]\n",
    "print('max sent length:', maxlen_sent, '\\nmax word length:', maxlen_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D, Input, Dense, Reshape, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import initializers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from util.util_functions import getWordIdx\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "gru_dim = 50\n",
    "dropout_rate = 0.2\n",
    "atten_dim = 50\n",
    "\n",
    "batch_size = 100\n",
    "epoch_num = 10\n",
    "\n",
    "categorical_label = True\n",
    "\n",
    "if categorical_label:\n",
    "    train_label_cat = np_utils.to_categorical(train_label)\n",
    "    test_label_cat = np_utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=100, kernel_size=3, strides=1, padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 112)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 112, 300)          2406900   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 112, 100)          90100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 2,497,000\n",
      "Trainable params: 90,100\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 13, 112)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 13, 100)           2497000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 13, 100)           45300     \n",
      "_________________________________________________________________\n",
      "att_layer_5 (AttLayer)       (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,547,602\n",
      "Trainable params: 140,702\n",
      "Non-trainable params: 2,406,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length= maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer = Convolution1D(nb_filter=100,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "#embedding matrix shape[1]是300，每个vector的维度\n",
    "max_pooling_layer = GlobalMaxPooling1D()\n",
    "\n",
    "\n",
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(maxlen_word,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn = cnn_layer(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn = max_pooling_layer(sent_cnn)  # output shape: (None, nb_filter)\n",
    "\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()\n",
    "\n",
    "# build document encoder model\n",
    "review_input = Input(shape=(maxlen_sent, maxlen_word), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)   # out shape: (None, MAX_SENTS, nb_filter)\n",
    "\n",
    "rnn_out = rnn_layer(review_encoder) # (batch_size, timesteps, gru_dimx2)\n",
    "\n",
    "att_out = AttLayer(atten_dim)(rnn_out)\n",
    "\n",
    "if categorical_label:\n",
    "    preds = Dense(2, activation='softmax')(att_out) # categorical output\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\n",
    "else:\n",
    "    preds = Dense(1, activation='sigmoid')(att_out)\n",
    "    model = Model(review_input, preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 25s 14ms/step - loss: 0.6969 - acc: 0.5117\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 7ms/step\n",
      "Accuracy: 0.4650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       107\n",
      "           1     0.4650    1.0000    0.6348        93\n",
      "\n",
      "   micro avg     0.4650    0.4650    0.4650       200\n",
      "   macro avg     0.2325    0.5000    0.3174       200\n",
      "weighted avg     0.2162    0.4650    0.2952       200\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 25s 14ms/step - loss: 0.6896 - acc: 0.5206\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.4650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.0093    0.0183       107\n",
      "           1     0.4646    0.9892    0.6323        93\n",
      "\n",
      "   micro avg     0.4650    0.4650    0.4650       200\n",
      "   macro avg     0.4823    0.4993    0.3253       200\n",
      "weighted avg     0.4836    0.4650    0.3038       200\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 25s 14ms/step - loss: 0.6856 - acc: 0.5428\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 6ms/step\n",
      "Accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5497    0.8785    0.6763       107\n",
      "           1     0.5517    0.1720    0.2623        93\n",
      "\n",
      "   micro avg     0.5500    0.5500    0.5500       200\n",
      "   macro avg     0.5507    0.5253    0.4693       200\n",
      "weighted avg     0.5506    0.5500    0.4838       200\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 26s 14ms/step - loss: 0.6755 - acc: 0.6256\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.4850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6250    0.0935    0.1626       107\n",
      "           1     0.4728    0.9355    0.6282        93\n",
      "\n",
      "   micro avg     0.4850    0.4850    0.4850       200\n",
      "   macro avg     0.5489    0.5145    0.3954       200\n",
      "weighted avg     0.5542    0.4850    0.3791       200\n",
      "\n",
      "Training for epoch 5/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 25s 14ms/step - loss: 0.6650 - acc: 0.6106\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5319    0.9346    0.6780       107\n",
      "           1     0.4167    0.0538    0.0952        93\n",
      "\n",
      "   micro avg     0.5250    0.5250    0.5250       200\n",
      "   macro avg     0.4743    0.4942    0.3866       200\n",
      "weighted avg     0.4783    0.5250    0.4070       200\n",
      "\n",
      "Training for epoch 6/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 26s 14ms/step - loss: 0.6036 - acc: 0.7100\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5977    0.4860    0.5361       107\n",
      "           1     0.5133    0.6237    0.5631        93\n",
      "\n",
      "   micro avg     0.5500    0.5500    0.5500       200\n",
      "   macro avg     0.5555    0.5548    0.5496       200\n",
      "weighted avg     0.5584    0.5500    0.5486       200\n",
      "\n",
      "Training for epoch 7/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 24s 13ms/step - loss: 0.5151 - acc: 0.7561\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5930    0.4766    0.5285       107\n",
      "           1     0.5088    0.6237    0.5604        93\n",
      "\n",
      "   micro avg     0.5450    0.5450    0.5450       200\n",
      "   macro avg     0.5509    0.5501    0.5444       200\n",
      "weighted avg     0.5538    0.5450    0.5433       200\n",
      "\n",
      "Training for epoch 8/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 24s 13ms/step - loss: 0.4342 - acc: 0.8128\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5701    0.5701    0.5701       107\n",
      "           1     0.5054    0.5054    0.5054        93\n",
      "\n",
      "   micro avg     0.5400    0.5400    0.5400       200\n",
      "   macro avg     0.5377    0.5377    0.5377       200\n",
      "weighted avg     0.5400    0.5400    0.5400       200\n",
      "\n",
      "Training for epoch 9/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 24s 13ms/step - loss: 0.3072 - acc: 0.8956\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5914    0.5140    0.5500       107\n",
      "           1     0.5140    0.5914    0.5500        93\n",
      "\n",
      "   micro avg     0.5500    0.5500    0.5500       200\n",
      "   macro avg     0.5527    0.5527    0.5500       200\n",
      "weighted avg     0.5554    0.5500    0.5500       200\n",
      "\n",
      "Training for epoch 10/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 26s 15ms/step - loss: 0.2309 - acc: 0.9372\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 6ms/step\n",
      "Accuracy: 0.5550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6047    0.4860    0.5389       107\n",
      "           1     0.5175    0.6344    0.5700        93\n",
      "\n",
      "   micro avg     0.5550    0.5550    0.5550       200\n",
      "   macro avg     0.5611    0.5602    0.5545       200\n",
      "weighted avg     0.5641    0.5550    0.5534       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-CNN+biGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", filters=50, kernel_size=3, strides=1, padding=\"same\")`\n",
      "C:\\Users\\wyl19\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\ipykernel\\__main__.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", weights=[array([[[..., trainable=False, filters=533, kernel_size=1, strides=1, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "# define some Keras layers\n",
    "embedding_layer = Embedding(vocab_size, embedding_matrix.shape[1], input_length=maxlen_word, \n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "cnn_layer1 = Convolution1D(nb_filter=50,\n",
    "                            filter_length=3,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                            subsample_length=1)\n",
    "\n",
    "cnn_layer2 = Convolution1D(nb_filter=CNN_weights[0].shape[2],\n",
    "                            filter_length=1,\n",
    "                            border_mode='same',\n",
    "                            activation='tanh',\n",
    "                           weights = CNN_weights,\n",
    "                           trainable = False,\n",
    "                            subsample_length=1)\n",
    "\n",
    "rnn_layer = Bidirectional(GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n",
    "# rnn_layer = GRU(gru_dim, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "\n",
    "max_pooling_layer = GlobalMaxPooling1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 112)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 112, 300)     2406900     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 112, 50)      45050       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 112, 533)     160433      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM multiple             0           conv1d_6[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 583)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_6[1][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,612,383\n",
      "Trainable params: 45,050\n",
      "Non-trainable params: 2,567,333\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build sentence encoder model\n",
    "sentence_input = Input(shape=(maxlen_word,), dtype='int32')\n",
    "\n",
    "sent_embedding = embedding_layer(sentence_input)  #input shape:(MAX_SENT_LENGTH),output shape:(MAX_SENT_LENGTH,embed dimension)\n",
    "\n",
    "sent_cnn1 = cnn_layer1(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn1 = max_pooling_layer(sent_cnn1)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn2 = cnn_layer2(sent_embedding) # output shape: (None, maxlen_word, nb_filter)\n",
    "# we use standard max over time pooling\n",
    "sent_cnn2 = max_pooling_layer(sent_cnn2)  # output shape: (None, nb_filter)\n",
    "\n",
    "sent_cnn = concatenate([sent_cnn1, sent_cnn2])\n",
    "\n",
    "sentEncoder = Model(sentence_input, sent_cnn)\n",
    "sentEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training for epoch 1/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 23s 13ms/step - loss: 0.1792 - acc: 0.9494\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5833    0.3925    0.4693       107\n",
      "           1     0.4922    0.6774    0.5701        93\n",
      "\n",
      "   micro avg     0.5250    0.5250    0.5250       200\n",
      "   macro avg     0.5378    0.5350    0.5197       200\n",
      "weighted avg     0.5410    0.5250    0.5162       200\n",
      "\n",
      "Training for epoch 2/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 27s 15ms/step - loss: 0.1620 - acc: 0.9561\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 5ms/step\n",
      "Accuracy: 0.5500\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5934    0.5047    0.5455       107\n",
      "           1     0.5138    0.6022    0.5545        93\n",
      "\n",
      "   micro avg     0.5500    0.5500    0.5500       200\n",
      "   macro avg     0.5536    0.5534    0.5500       200\n",
      "weighted avg     0.5564    0.5500    0.5496       200\n",
      "\n",
      "Training for epoch 3/10\n",
      "Epoch 1/1\n",
      "1800/1800 [==============================] - 28s 16ms/step - loss: 0.1737 - acc: 0.9344\n",
      "Evaluating...\n",
      "200/200 [==============================] - 1s 6ms/step\n",
      "Accuracy: 0.4850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5526    0.1963    0.2897       107\n",
      "           1     0.4691    0.8172    0.5961        93\n",
      "\n",
      "   micro avg     0.4850    0.4850    0.4850       200\n",
      "   macro avg     0.5109    0.5067    0.4429       200\n",
      "weighted avg     0.5138    0.4850    0.4321       200\n",
      "\n",
      "Training for epoch 4/10\n",
      "Epoch 1/1\n",
      " 200/1800 [==>...........................] - ETA: 24s - loss: 0.2294 - acc: 0.9300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1b9af32563a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training for epoch {}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcategorical_label\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_copus_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_copus_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "for i in range(epoch_num):\n",
    "    print('Training for epoch {}/{}'.format(i+1,epoch_num))\n",
    "    if categorical_label:\n",
    "        model.fit(train_copus_padded, train_label_cat, batch_size=batch_size,epochs=1)\n",
    "    else:\n",
    "        model.fit(train_copus_padded, train_label, batch_size=batch_size,epochs=1)\n",
    "        \n",
    "    print('Evaluating...')\n",
    "    pred_test_prob = model.predict(test_copus_padded, batch_size=batch_size, verbose=True)\n",
    "    # predict the class label\n",
    "    if pred_test_prob.shape[-1]>1:\n",
    "        pred_test = pred_test_prob.argmax(axis=-1)\n",
    "    else:\n",
    "        pred_test = (pred_test_prob>0.5).astype('int32')\n",
    "        pred_test = pred_test.reshape(pred_test.shape[0])\n",
    "\n",
    "    acc = np.sum(pred_test == test_label) / float(len(test_label))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % (acc))   \n",
    "    print(classification_report(test_label, pred_test, digits=4, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biGRU+CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+biGRU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
